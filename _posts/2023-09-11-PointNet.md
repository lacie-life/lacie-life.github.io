---
title: Paper note 3 - PointNets Series
# author:
#   name: Life Zero
#   link: https://github.com/lacie-life
date:  2023-09-11 11:11:14 +0700
categories: [Computer Vision]
tags: [Paper]
img_path: /assets/img/post_assest/pvo/
render_with_liquid: false
---

# 1. PointNet - Deep Learning on Point Sets for 3D Classification and Segmentation

Point cloud is an important type of geometric data
structure. Due to its irregular format, most researchers
transform such data to regular 3D voxel grids or collections
of images. This, however, renders data unnecessarily
voluminous and causes issues. In this paper, we design a
novel type of neural network that directly consumes point
clouds, which well respects the permutation invariance of
points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from
object classification, part segmentation, to scene semantic
parsing. Though simple, PointNet is highly efficient and
effective. Empirically, it shows strong performance on
par or even better than state of the art. Theoretically,
we provide analysis towards understanding of what the
network has learnt and why the network is robust with
respect to input perturbation and corruption.

![PointNet Architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-1.png?raw=true)

## 1.1. Problem Statement

We design a deep learning framework that directly
consumes unordered point sets as inputs. A point cloud is
represented as a set of 3D points $\{P_i
| i = 1, ..., n\}$, where
each point $P_i$
is a vector of its $(x, y, z)$ coordinate plus extra
feature channels such as color, normal etc. For simplicity
and clarity, unless otherwise noted, we only use the $(x, y, z)$
coordinate as our point’s channels.

For the object classification task, the input point cloud is
either directly sampled from a shape or pre-segmented from
a scene point cloud. Our proposed deep network outputs
$k$ scores for all the $k$ candidate classes. For semantic
segmentation, the input can be a single object for part region
segmentation, or a sub-volume from a 3D scene for object
region segmentation. Our model will output $n$ × $m$ scores
for each of the $n$ points and each of the $m$ semantic subcategories.

## 1.2. Deep Learning on Point Sets

### 1.2.1. Properties of Point Sets in $R^n$

Our input is a subset of points from an Euclidean space.
It has three main properties:

• <b> Unordered: </b> Unlike pixel arrays in images or voxel
arrays in volumetric grids, point cloud is a set of points
without specific order. In other words, a network that
consumes N 3D point sets needs to be invariant to N!
permutations of the input set in data feeding order.

• <b> Interaction among points: </b> The points are from a space
with a distance metric. It means that points are not
isolated, and neighboring points form a meaningful
subset. Therefore, the model needs to be able to
capture local structures from nearby points, and the
combinatorial interactions among local structures.

• <b> Invariance under transformations: </b> As a geometric
object, the learned representation of the point set
should be invariant to certain transformations. For
example, rotating and translating points all together
should not modify the global point cloud category nor
the segmentation of the points.

### 1.2.2. PointNet Architecture

PointNet has three key modules: the max pooling
layer as a symmetric function to aggregate information from all the points, a local and global information combination
structure, and two joint alignment networks that align both
input points and point features.

<b> 1. Symmetry Function for Unordered Input </b>

In order to make a model invariant to input permutation, three
strategies exist: 

- Sort input into a canonical order

- Treat the input as a sequence to train an RNN, but augment the
training data by all kinds of permutations

- Use a simple symmetric function to aggregate the information from each
point. Here, a symmetric function takes n vectors as input
and outputs a new vector that is invariant to the input
order. For example, + and ∗ operators are symmetric binary
functions.

While sorting sounds like a simple solution, in high
dimensional space there in fact does not exist an ordering
that is stable w.r.t. point perturbations in the general
sense. This can be easily shown by contradiction. If
such an ordering strategy exists, it defines a bijection map
between a high-dimensional space and a 1d real line. It
is not hard to see, to require an ordering to be stable w.r.t
point perturbations is equivalent to requiring that this map
preserves spatial proximity as the dimension reduces, a task
that cannot be achieved in the general case. Therefore,
sorting does not fully resolve the ordering issue, and it’s
hard for a network to learn a consistent mapping from
input to output as the ordering issue persists. As shown in
experiments, we find that applying a MLP directly
on the sorted point set performs poorly, though slightly
better than directly processing an unsorted input.

RNN not good.

![Three approaches to achieve order invariance](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-2.png?raw=true)

=> approximate a general function defined on
a point set by applying a symmetric function on transformed
elements in the set.

![symmetric function](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-3.png?raw=true)

Empirically, our basic module is very simple: we
approximate $h$ by a multi-layer perceptron network and
$g$ by a composition of a single variable function and a
max pooling function. This is found to work well by
experiments. Through a collection of $h$, we can learn a
number of $f$’s to capture different properties of the set.

<b> 2. Local and Global Information Aggregation </b>

The output
from the above section forms a vector $[f_1, . . . , f_K]$, which
is a global signature of the input set. We can easily
train a SVM or multi-layer perceptron classifier on the
shape global features for classification. However, point
segmentation requires a combination of local and global
knowledge. We can achieve this by a simple yet highly
effective manner.

Our solution can be seen in Segmentation Network part. After computing the global point cloud feature vector, we feed it back to per point features by concatenating
the global feature with each of the point features. Then we
extract new per point features based on the combined point
features - this time the per point feature is aware of both the
local and global information.

![PointNet Architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-1.png?raw=true)

=> With this modification our network is <b> able to predict
per point quantities that rely on both local geometry and
global semantics </b>. 

For example we can accurately predict
per-point normals (fig in supplementary), validating that the
network is able to summarize information from the point’s
local neighborhood.

<b> 3. Joint Alignment Network </b>

The semantic labeling of a
point cloud has to be invariant if the point cloud undergoes
certain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by
our point set is invariant to these transformations.

Our input form of point clouds allows us to achieve this
goal in a much simpler way. We do not
need to invent any new layers and no alias is introduced as in
the image case. We predict an affine transformation matrix
by a mini-network (T-net) and directly apply this
transformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed
by basic modules of point independent feature extraction,
max pooling and fully connected layers. More details about
the T-net are in the supplementary.
This idea can be further extended to the alignment of
feature space, as well. We can insert another alignment network on point features and predict a feature transformation
matrix to align features from different input point clouds.
However, transformation matrix in the feature space has
much higher dimension than the spatial transform matrix,
which greatly increases the difficulty of optimization. We
therefore add a regularization term to our softmax training
loss. We constrain the feature transformation matrix to be
close to orthogonal matrix:

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-4.png?raw=true)

where $A$ is the feature alignment matrix predicted by a
mini-network. An orthogonal transformation will not lose
information in the input, thus is desired. We find that by
adding the regularization term, the optimization becomes
more stable and our model achieves better performance.

<b> Intuitively, PointNet learns to summarize a shape by
a sparse set of key points </b>

## 1.3. Experiments

- [Code](https://github.com/charlesq34/pointnet)

# 2. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space

## 2.1. PointNet nad PointNet++

The basic idea of PointNet is to learn a spatial encoding of each point and then
aggregate all individual point features to a global point cloud signature. By its design, PointNet does
not capture local structure induced by the metric. However, exploiting local structure has proven to
be important for the success of convolutional architectures. A CNN takes data defined on regular
grids as the input and is able to progressively capture features at increasingly larger scales along a
multi-resolution hierarchy. At lower levels neurons have smaller receptive fields whereas at higher
levels they have larger receptive fields. The ability to abstract local patterns along the hierarchy
allows better generalizability to unseen cases.

We introduce a hierarchical neural network, named as PointNet++, to process a set of points sampled
in a metric space in a hierarchical fashion. The general idea of PointNet++ is simple. We first
partition the set of points into overlapping local regions by the distance metric of the underlying
space. Similar to CNNs, we extract local features capturing fine geometric structures from small
neighborhoods; such local features are further grouped into larger units and processed to produce
higher level features. This process is repeated until we obtain the features of the whole point set.

The design of PointNet++ has to address two issues: how to generate the partitioning of the point set,
and how to abstract sets of points or local features through a local feature learner. The two issues
are correlated because the partitioning of the point set has to produce common structures across
partitions, so that weights of local feature learners can be shared, as in the convolutional setting. We
choose our local feature learner to be PointNet. As demonstrated in that work, PointNet is an effective
architecture to process an unordered set of points for semantic feature extraction. In addition, this
architecture is robust to input data corruption. As a basic building block, PointNet abstracts sets of
local points or features into higher level representations. In this view, PointNet++ applies PointNet
recursively on a nested partitioning of the input set.

One issue that still remains is how to generate
overlapping partitioning of a point set. Each
partition is defined as a neighborhood ball in
the underlying Euclidean space, whose parameters include centroid location and scale. To
evenly cover the whole set, the centroids are selected among input point set by a farthest point
sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed
strides, our local receptive fields are dependent
on both the input data and the metric, and thus
more efficient and effective.

A significant contribution of this paper is that PointNet++ leverages neighborhoods at multiple scales
to achieve both robustness and detail capture. Assisted with random input dropout during training,
the network learns to adaptively weight patterns detected at different scales and combine multi-scale
features according to the input data. 

## 2.2. Problem Statement

Suppose that $X = (M, d)$ is a discrete metric space whose metric is inherited from a Euclidean space
$R^n$, where $M ⊆ R^n$ is the set of points and d is the distance metric. In addition, the density of $M$
in the ambient Euclidean space may not be uniform everywhere. We are interested in learning set
functions $f$ that take such $X$ as the input (along with additional features for each point) and produce
information of semantic interest regrading $X$ . In practice, such $f$ can be classification function that
assigns a label to $X$ or a segmentation function that assigns a per point label to each member of $M$.

## 2.3. Method

### 2.3.1. Hierarchical Point Set Feature Learning

While PointNet uses a single max pooling operation to aggregate the whole point set, our new
architecture builds a hierarchical grouping of points and progressively abstract larger and larger local
regions along the hierarchy.
Our hierarchical structure is composed by a number of set abstraction levels. At each level, a
set of points is processed and abstracted to produce a new set with fewer elements. The set abstraction
level is made of three key layers: Sampling layer, Grouping layer and PointNet layer. The Sampling
layer selects a set of points from input points, which defines the centroids of local regions. Grouping
layer then constructs local region sets by finding “neighboring” points around the centroids. PointNet
layer uses a mini-PointNet to encode local region patterns into feature vectors.

![Hierarchical feature learning architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-5.png?raw=true)

- <b> Sampling layer: </b> Given input points $\{x_1, x_2, ..., x_n\}$, we use iterative farthest point sampling (FPS)
to choose a subset of points $\{x_{i_1}, x_{i_2}, ..., x_{i_m}\}$, such that $x_{i_j}$
is the most distant point (in metric
distance) from the set $\{x_{i_1}, x_{i_2}, ..., x_{i_{j−1}}\}$ with regard to the rest points. Compared with random
sampling, it has better coverage of the entire point set given the same number of centroids. In contrast
to CNNs that scan the vector space agnostic of data distribution, our sampling strategy generates
receptive fields in a data dependent manner.

- <b> Grouping layer: </b> The input to this layer is a point set of size $N × (d + C)$ and the coordinates of
a set of centroids of size $N' × d$. The output are groups of point sets of size $N' × K × (d + C)$,
where each group corresponds to a local region and $K$ is the number of points in the neighborhood of
centroid points. Note that $K$ varies across groups but the succeeding PointNet layer is able to convert
flexible number of points into a fixed length local region feature vector.

- <b> PointNet layer: </b> In this layer, the input are $N'$
local regions of points with data size $N'×K×(d+C)$.
Each local region in the output is abstracted by its centroid and local feature that encodes the centroid’s
neighborhood. Output data size is $N' × (d + C')$.

The coordinates of points in a local region are firstly translated into a local frame relative to the
centroid point: $x^{(j)}_i = x^{(j)}_i − \hat x^((j))$
for $i = 1, 2, ..., K$ and $j = 1, 2, ..., d$ where $\hat x$ is the coordinate of
the centroid. We use PointNet as the basic building block for local pattern
learning. By using relative coordinates together with point features we can capture point-to-point
relations in the local region.

### 2.3.2.  Robust Feature Learning under Non-Uniform Sampling Density


As discussed earlier, it is common that <b> a point set comes with nonuniform density in different areas </b>. Such non-uniformity introduces
a significant challenge for point set feature learning. Features learned
in dense data may not generalize to sparsely sampled regions. Consequently, <b> models trained for sparse point cloud may not recognize
fine-grained local structures </b>.

Ideally, we want to <b> inspect as closely as possible into a point set
to capture finest details in densely sampled regions </b>. However, such
close inspect is prohibited at low density areas because <b> local patterns
may be corrupted by the sampling deficiency </b>. In this case, we should
<b> look for larger scale patterns in greater vicinity </b>. To achieve this goal
we propose density adaptive PointNet layers that learn to
<b> combine features from regions of different scales when the input sampling density changes </b>. We call
our hierarchical network with density adaptive PointNet layers as <b> PointNet++ </b>.

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-6.png?raw=true)

Previously section, each abstraction level contains grouping and feature extraction of a single scale.
In PointNet++, each abstraction level extracts multiple scales of local patterns and combine them
intelligently according to local point densities. In terms of grouping local regions and combining
features from different scales, we propose two types of density adaptive layers as listed below.

 - <b> Multi-scale grouping (MSG): </b> A simple but effective way to capture multiscale patterns is to apply grouping layers with different scales followed by according PointNets to
extract features of each scale. Features at different scales are concatenated to form a multi-scale
feature.

We train the network to learn an optimized strategy to combine the multi-scale features. This is done
by randomly dropping out input points with a randomized probability for each instance, which we call
random input dropout. Specifically, for each training point set, we choose a dropout ratio $θ$ uniformly
sampled from $[0, p]$ where $p ≤ 1$. For each point, we randomly drop a point with probability $θ$. In
practice we set $p = 0.95$ to avoid generating empty point sets. In doing so we present the network
with training sets of various sparsity (induced by $θ$) and varying uniformity (induced by randomness
in dropout). During test, we keep all available points.

- <b> Multi-resolution grouping (MRG): </b> The MSG approach above is computationally expensive since
it runs local PointNet at large scale neighborhoods for every centroid point. In particular, since the
number of centroid points is usually quite large at the lowest level, the time cost is significant.

## 2.3.3. Point Feature Propagation for Set Segmentation

In set abstraction layer, the original point set is subsampled. However in set segmentation task such
as semantic point labeling, we want to obtain point features for all the original points. One solution is
to always sample all points as centroids in all set abstraction levels, which however results in high
computation cost. Another way is to propagate features from subsampled points to the original points.

We adopt a hierarchical propagation strategy with distance based interpolation and across level
skip links.

In a feature propagation level, we propagate point features from
$N_l × (d + C)$ points to $N_{l−1}$ points where $N_{l−1}$ and $N_l$ (with $N_l ≤ N_{l−1}$) are point set size of input
and output of set abstraction level $l$. We achieve feature propagation by interpolating feature values
f of $N_l$ points at coordinates of the $N_{l−1}$ points. Among the many choices for interpolation, we
use inverse distance weighted average based on k nearest neighbors (in default we use
$p = 2, k = 3$). The interpolated features on $N_{l−1}$ points are then concatenated with skip linked point
features from the set abstraction level. Then the concatenated features are passed through a <b>“unit
pointnet” </b>, which is similar to one-by-one convolution in CNNs. A few shared fully connected and
ReLU layers are applied to update each point’s feature vector. The process is repeated until we have
propagated features to the original set of points.

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-7.png?raw=true)

## 2.4. Experiments

- [Code](https://github.com/charlesq34/pointnet2)

# 3. Frustum PointNets for 3D Object Detection from RGB-D Data












