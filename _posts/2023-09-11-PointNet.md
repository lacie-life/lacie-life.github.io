---
title: Paper note 3 - PointNets Series
# author:
#   name: Life Zero
#   link: https://github.com/lacie-life
date:  2023-09-11 11:11:14 +0700
categories: [Computer Vision]
tags: [Paper]
img_path: /assets/img/post_assest/pvo/
render_with_liquid: false
---

# 1. PointNet - Deep Learning on Point Sets for 3D Classification and Segmentation

Point cloud is an important type of geometric data
structure. Due to its irregular format, most researchers
transform such data to regular 3D voxel grids or collections
of images. This, however, renders data unnecessarily
voluminous and causes issues. In this paper, we design a
novel type of neural network that directly consumes point
clouds, which well respects the permutation invariance of
points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from
object classification, part segmentation, to scene semantic
parsing. Though simple, PointNet is highly efficient and
effective. Empirically, it shows strong performance on
par or even better than state of the art. Theoretically,
we provide analysis towards understanding of what the
network has learnt and why the network is robust with
respect to input perturbation and corruption.

![PointNet Architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-1.png?raw=true)

## 1.1. Problem Statement

We design a deep learning framework that directly
consumes unordered point sets as inputs. A point cloud is
represented as a set of 3D points $\{P_i
| i = 1, ..., n\}$, where
each point $P_i$
is a vector of its $(x, y, z)$ coordinate plus extra
feature channels such as color, normal etc. For simplicity
and clarity, unless otherwise noted, we only use the $(x, y, z)$
coordinate as our point’s channels.

For the object classification task, the input point cloud is
either directly sampled from a shape or pre-segmented from
a scene point cloud. Our proposed deep network outputs
$k$ scores for all the $k$ candidate classes. For semantic
segmentation, the input can be a single object for part region
segmentation, or a sub-volume from a 3D scene for object
region segmentation. Our model will output $n$ × $m$ scores
for each of the $n$ points and each of the $m$ semantic subcategories.

## 1.2. Deep Learning on Point Sets

### 1.2.1. Properties of Point Sets in $R^n$

Our input is a subset of points from an Euclidean space.
It has three main properties:

• <b> Unordered: </b> Unlike pixel arrays in images or voxel
arrays in volumetric grids, point cloud is a set of points
without specific order. In other words, a network that
consumes N 3D point sets needs to be invariant to N!
permutations of the input set in data feeding order.

• <b> Interaction among points: </b> The points are from a space
with a distance metric. It means that points are not
isolated, and neighboring points form a meaningful
subset. Therefore, the model needs to be able to
capture local structures from nearby points, and the
combinatorial interactions among local structures.

• <b> Invariance under transformations: </b> As a geometric
object, the learned representation of the point set
should be invariant to certain transformations. For
example, rotating and translating points all together
should not modify the global point cloud category nor
the segmentation of the points.

### 1.2.2. PointNet Architecture

PointNet has three key modules: the max pooling
layer as a symmetric function to aggregate information from all the points, a local and global information combination
structure, and two joint alignment networks that align both
input points and point features.

<b> 1. Symmetry Function for Unordered Input </b>

In order to make a model invariant to input permutation, three
strategies exist: 

- Sort input into a canonical order

- Treat the input as a sequence to train an RNN, but augment the
training data by all kinds of permutations

- Use a simple symmetric function to aggregate the information from each
point. Here, a symmetric function takes n vectors as input
and outputs a new vector that is invariant to the input
order. For example, + and ∗ operators are symmetric binary
functions.

While sorting sounds like a simple solution, in high
dimensional space there in fact does not exist an ordering
that is stable w.r.t. point perturbations in the general
sense. This can be easily shown by contradiction. If
such an ordering strategy exists, it defines a bijection map
between a high-dimensional space and a 1d real line. It
is not hard to see, to require an ordering to be stable w.r.t
point perturbations is equivalent to requiring that this map
preserves spatial proximity as the dimension reduces, a task
that cannot be achieved in the general case. Therefore,
sorting does not fully resolve the ordering issue, and it’s
hard for a network to learn a consistent mapping from
input to output as the ordering issue persists. As shown in
experiments, we find that applying a MLP directly
on the sorted point set performs poorly, though slightly
better than directly processing an unsorted input.

RNN not good.

![Three approaches to achieve order invariance](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-2.png?raw=true)

=> approximate a general function defined on
a point set by applying a symmetric function on transformed
elements in the set.

![symmetric function](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-3.png?raw=true)

Empirically, our basic module is very simple: we
approximate $h$ by a multi-layer perceptron network and
$g$ by a composition of a single variable function and a
max pooling function. This is found to work well by
experiments. Through a collection of $h$, we can learn a
number of $f$’s to capture different properties of the set.

<b> 2. Local and Global Information Aggregation </b>

The output
from the above section forms a vector $[f_1, . . . , f_K]$, which
is a global signature of the input set. We can easily
train a SVM or multi-layer perceptron classifier on the
shape global features for classification. However, point
segmentation requires a combination of local and global
knowledge. We can achieve this by a simple yet highly
effective manner.

Our solution can be seen in Segmentation Network part. After computing the global point cloud feature vector, we feed it back to per point features by concatenating
the global feature with each of the point features. Then we
extract new per point features based on the combined point
features - this time the per point feature is aware of both the
local and global information.

![PointNet Architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-1.png?raw=true)

=> With this modification our network is <b> able to predict
per point quantities that rely on both local geometry and
global semantics </b>. 

For example we can accurately predict
per-point normals (fig in supplementary), validating that the
network is able to summarize information from the point’s
local neighborhood.

<b> 3. Joint Alignment Network </b>

The semantic labeling of a
point cloud has to be invariant if the point cloud undergoes
certain geometric transformations, such as rigid transformation. We therefore expect that the learnt representation by
our point set is invariant to these transformations.

Our input form of point clouds allows us to achieve this
goal in a much simpler way. We do not
need to invent any new layers and no alias is introduced as in
the image case. We predict an affine transformation matrix
by a mini-network (T-net) and directly apply this
transformation to the coordinates of input points. The mininetwork itself resembles the big network and is composed
by basic modules of point independent feature extraction,
max pooling and fully connected layers. More details about
the T-net are in the supplementary.
This idea can be further extended to the alignment of
feature space, as well. We can insert another alignment network on point features and predict a feature transformation
matrix to align features from different input point clouds.
However, transformation matrix in the feature space has
much higher dimension than the spatial transform matrix,
which greatly increases the difficulty of optimization. We
therefore add a regularization term to our softmax training
loss. We constrain the feature transformation matrix to be
close to orthogonal matrix:

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-4.png?raw=true)

where $A$ is the feature alignment matrix predicted by a
mini-network. An orthogonal transformation will not lose
information in the input, thus is desired. We find that by
adding the regularization term, the optimization becomes
more stable and our model achieves better performance.

<b> Intuitively, PointNet learns to summarize a shape by
a sparse set of key points </b>

## 1.3. Experiments

- [Code](https://github.com/charlesq34/pointnet)

# 2. PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space

## 2.1. PointNet nad PointNet++

The basic idea of PointNet is to learn a spatial encoding of each point and then
aggregate all individual point features to a global point cloud signature. By its design, PointNet does
not capture local structure induced by the metric. However, exploiting local structure has proven to
be important for the success of convolutional architectures. A CNN takes data defined on regular
grids as the input and is able to progressively capture features at increasingly larger scales along a
multi-resolution hierarchy. At lower levels neurons have smaller receptive fields whereas at higher
levels they have larger receptive fields. The ability to abstract local patterns along the hierarchy
allows better generalizability to unseen cases.

We introduce a hierarchical neural network, named as PointNet++, to process a set of points sampled
in a metric space in a hierarchical fashion. The general idea of PointNet++ is simple. We first
partition the set of points into overlapping local regions by the distance metric of the underlying
space. Similar to CNNs, we extract local features capturing fine geometric structures from small
neighborhoods; such local features are further grouped into larger units and processed to produce
higher level features. This process is repeated until we obtain the features of the whole point set.

The design of PointNet++ has to address two issues: how to generate the partitioning of the point set,
and how to abstract sets of points or local features through a local feature learner. The two issues
are correlated because the partitioning of the point set has to produce common structures across
partitions, so that weights of local feature learners can be shared, as in the convolutional setting. We
choose our local feature learner to be PointNet. As demonstrated in that work, PointNet is an effective
architecture to process an unordered set of points for semantic feature extraction. In addition, this
architecture is robust to input data corruption. As a basic building block, PointNet abstracts sets of
local points or features into higher level representations. In this view, PointNet++ applies PointNet
recursively on a nested partitioning of the input set.

One issue that still remains is how to generate
overlapping partitioning of a point set. Each
partition is defined as a neighborhood ball in
the underlying Euclidean space, whose parameters include centroid location and scale. To
evenly cover the whole set, the centroids are selected among input point set by a farthest point
sampling (FPS) algorithm. Compared with volumetric CNNs that scan the space with fixed
strides, our local receptive fields are dependent
on both the input data and the metric, and thus
more efficient and effective.

A significant contribution of this paper is that PointNet++ leverages neighborhoods at multiple scales
to achieve both robustness and detail capture. Assisted with random input dropout during training,
the network learns to adaptively weight patterns detected at different scales and combine multi-scale
features according to the input data. 

## 2.2. Problem Statement

Suppose that $X = (M, d)$ is a discrete metric space whose metric is inherited from a Euclidean space
$R^n$, where $M ⊆ R^n$ is the set of points and d is the distance metric. In addition, the density of $M$
in the ambient Euclidean space may not be uniform everywhere. We are interested in learning set
functions $f$ that take such $X$ as the input (along with additional features for each point) and produce
information of semantic interest regrading $X$ . In practice, such $f$ can be classification function that
assigns a label to $X$ or a segmentation function that assigns a per point label to each member of $M$.

## 2.3. Method

### 2.3.1. Hierarchical Point Set Feature Learning

While PointNet uses a single max pooling operation to aggregate the whole point set, our new
architecture builds a hierarchical grouping of points and progressively abstract larger and larger local
regions along the hierarchy.
Our hierarchical structure is composed by a number of set abstraction levels. At each level, a
set of points is processed and abstracted to produce a new set with fewer elements. The set abstraction
level is made of three key layers: Sampling layer, Grouping layer and PointNet layer. The Sampling
layer selects a set of points from input points, which defines the centroids of local regions. Grouping
layer then constructs local region sets by finding “neighboring” points around the centroids. PointNet
layer uses a mini-PointNet to encode local region patterns into feature vectors.

![Hierarchical feature learning architecture](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-5.png?raw=true)

- <b> Sampling layer: </b> Given input points $\{x_1, x_2, ..., x_n\}$, we use iterative farthest point sampling (FPS)
to choose a subset of points $\{x_{i_1}, x_{i_2}, ..., x_{i_m}\}$, such that $x_{i_j}$
is the most distant point (in metric
distance) from the set $\{x_{i_1}, x_{i_2}, ..., x_{i_{j−1}}\}$ with regard to the rest points. Compared with random
sampling, it has better coverage of the entire point set given the same number of centroids. In contrast
to CNNs that scan the vector space agnostic of data distribution, our sampling strategy generates
receptive fields in a data dependent manner.

- <b> Grouping layer: </b> The input to this layer is a point set of size $N × (d + C)$ and the coordinates of
a set of centroids of size $N' × d$. The output are groups of point sets of size $N' × K × (d + C)$,
where each group corresponds to a local region and $K$ is the number of points in the neighborhood of
centroid points. Note that $K$ varies across groups but the succeeding PointNet layer is able to convert
flexible number of points into a fixed length local region feature vector.

- <b> PointNet layer: </b> In this layer, the input are $N'$
local regions of points with data size $N'×K×(d+C)$.
Each local region in the output is abstracted by its centroid and local feature that encodes the centroid’s
neighborhood. Output data size is $N' × (d + C')$.

The coordinates of points in a local region are firstly translated into a local frame relative to the
centroid point: $x^{(j)}_i = x^{(j)}_i − \hat x^((j))$
for $i = 1, 2, ..., K$ and $j = 1, 2, ..., d$ where $\hat x$ is the coordinate of
the centroid. We use PointNet as the basic building block for local pattern
learning. By using relative coordinates together with point features we can capture point-to-point
relations in the local region.

### 2.3.2.  Robust Feature Learning under Non-Uniform Sampling Density


As discussed earlier, it is common that <b> a point set comes with nonuniform density in different areas </b>. Such non-uniformity introduces
a significant challenge for point set feature learning. Features learned
in dense data may not generalize to sparsely sampled regions. Consequently, <b> models trained for sparse point cloud may not recognize
fine-grained local structures </b>.

Ideally, we want to <b> inspect as closely as possible into a point set
to capture finest details in densely sampled regions </b>. However, such
close inspect is prohibited at low density areas because <b> local patterns
may be corrupted by the sampling deficiency </b>. In this case, we should
<b> look for larger scale patterns in greater vicinity </b>. To achieve this goal
we propose density adaptive PointNet layers that learn to
<b> combine features from regions of different scales when the input sampling density changes </b>. We call
our hierarchical network with density adaptive PointNet layers as <b> PointNet++ </b>.

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-6.png?raw=true)

Previously section, each abstraction level contains grouping and feature extraction of a single scale.
In PointNet++, each abstraction level extracts multiple scales of local patterns and combine them
intelligently according to local point densities. In terms of grouping local regions and combining
features from different scales, we propose two types of density adaptive layers as listed below.

 - <b> Multi-scale grouping (MSG): </b> A simple but effective way to capture multiscale patterns is to apply grouping layers with different scales followed by according PointNets to
extract features of each scale. Features at different scales are concatenated to form a multi-scale
feature.

We train the network to learn an optimized strategy to combine the multi-scale features. This is done
by randomly dropping out input points with a randomized probability for each instance, which we call
random input dropout. Specifically, for each training point set, we choose a dropout ratio $θ$ uniformly
sampled from $[0, p]$ where $p ≤ 1$. For each point, we randomly drop a point with probability $θ$. In
practice we set $p = 0.95$ to avoid generating empty point sets. In doing so we present the network
with training sets of various sparsity (induced by $θ$) and varying uniformity (induced by randomness
in dropout). During test, we keep all available points.

- <b> Multi-resolution grouping (MRG): </b> The MSG approach above is computationally expensive since
it runs local PointNet at large scale neighborhoods for every centroid point. In particular, since the
number of centroid points is usually quite large at the lowest level, the time cost is significant.

## 2.3.3. Point Feature Propagation for Set Segmentation

In set abstraction layer, the original point set is subsampled. However in set segmentation task such
as semantic point labeling, we want to obtain point features for all the original points. One solution is
to always sample all points as centroids in all set abstraction levels, which however results in high
computation cost. Another way is to propagate features from subsampled points to the original points.

We adopt a hierarchical propagation strategy with distance based interpolation and across level
skip links.

In a feature propagation level, we propagate point features from
$N_l × (d + C)$ points to $N_{l−1}$ points where $N_{l−1}$ and $N_l$ (with $N_l ≤ N_{l−1}$) are point set size of input
and output of set abstraction level $l$. We achieve feature propagation by interpolating feature values
f of $N_l$ points at coordinates of the $N_{l−1}$ points. Among the many choices for interpolation, we
use inverse distance weighted average based on k nearest neighbors (in default we use
$p = 2, k = 3$). The interpolated features on $N_{l−1}$ points are then concatenated with skip linked point
features from the set abstraction level. Then the concatenated features are passed through a <b>“unit
pointnet” </b>, which is similar to one-by-one convolution in CNNs. A few shared fully connected and
ReLU layers are applied to update each point’s feature vector. The process is repeated until we have
propagated features to the original set of points.

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-7.png?raw=true)

## 2.4. Experiments

- [Code](https://github.com/charlesq34/pointnet2)

# 3. Frustum PointNets for 3D Object Detection from RGB-D Data

While <b> PointNets are capable of classifying a whole point
cloud or predicting a semantic class for each point in a point
cloud </b> , it is <b> unclear how this architecture can be used for
instance-level 3D object detection </b>. Towards this goal, we
have to address one key challenge: <b> how to efficiently propose possible locations of 3D objects in a 3D space </b>. Imitating the practice in image detection, it is straightforward
to enumerate candidate 3D boxes by sliding windows
or by 3D region proposal networks. However,
the computational complexity of 3D search typically grows
cubically with respect to resolution and becomes too expensive for large scenes or real-time applications such as
autonomous driving.

Instead, in this work, we reduce the search space following the dimension reduction principle: we take the advantage of mature 2D object detectors. First, we
extract the 3D bounding frustum of an object by extruding
2D bounding boxes from image detectors. Then, within the
3D space trimmed by each of the 3D frustums, we consecutively perform 3D object instance segmentation and amodal 3D bounding box regression using two variants of PointNet. The segmentation network predicts the 3D mask of
the object of interest (i.e. instance segmentation); and the
regression network estimates the amodal 3D bounding box
(covering the entire object even if only part of it is visible).

In contrast to previous work that treats RGB-D data as
2D maps for CNNs, our method is more 3D-centric as we
lift depth maps to 3D point clouds and process them using 3D tools. This 3D-centric view enables new capabilities
for exploring 3D data in a more effective manner. First,
in our pipeline, a few transformations are applied successively on 3D coordinates, which align point clouds into a
sequence of more constrained and canonical frames. These
alignments factor out pose variations in data, and thus make
3D geometry pattern more evident, leading to an easier job
of 3D learners. Second, learning in 3D space can better exploits the geometric and topological structure of 3D space.
In principle, all objects live in 3D space; therefore, we believe that many geometric structures, such as repetition, planarity, and symmetry, are more naturally parameterized and
captured by learners that directly operate in 3D space. The
usefulness of this 3D-centric network design philosophy has
been supported by much recent experimental evidence.

![3D object detection pipeline](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-8.png?raw=true)

While PointNets
have been applied to single object classification and semantic segmentation, our work explores how to extend the architecture for the purpose of <b> 3D object detection </b>.

## 3.1. Problem Statement

Given RGB-D data as input, our goal is to classify and
localize objects in 3D space. The depth data, obtained from
LiDAR or indoor depth sensors, is represented as a point
cloud in RGB camera coordinates. The projection matrix
is also known so that we can get a 3D frustum from a 2D
image region. Each object is represented by a class (one
among $k$ predefined classes) and an <b> amodal </b> 3D bounding
box. The <b> amodal </b> box bounds the complete object even if
part of the object is occluded or truncated. The 3D box is
parameterized by its size $h$, $w$, $l$, center $c_x$, $c_y$, $c_z$, and orientation $θ$, $φ$, $ψ$ relative to a predefined canonical pose for
each category. In our implementation, we only consider the
heading angle $θ$ around the up-axis for orientation.

## 3.2. 3D Detection with Frustum PointNets

Our system for 3D object detection
consists of three modules: frustum proposal, 3D instance
segmentation, and 3D amodal bounding box estimation. We
will introduce each module in the following subsections.
We will focus on the pipeline and functionality of each module, and refer readers to supplementary for specific architectures of the deep networks involved.

![Frustum PointNets for 3D object detection](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-9.png?raw=true)

### 3.2.1. Frustum Proposal

The resolution of data produced by most 3D sensors, especially real-time depth sensors, is still lower than RGB
images from commodity cameras. Therefore, we leverage
mature 2D object detector to propose 2D object regions in
RGB images as well as to classify objects.
With a known camera projection matrix, a 2D bounding
box can be lifted to a frustum (with near and far planes specified by depth sensor range) that defines a 3D search space
for the object. We then collect all points within the frustum
to form a <b> frustum point cloud </b>. As shown in Fig 4 (a), frustums may orient towards many different directions, which
result in large variation in the placement of point clouds.
We therefore normalize the frustums by rotating them toward a center view such that the center axis of the frustum is
orthogonal to the image plane. This normalization helps improve the rotation-invariance of the algorithm. We call this
entire procedure for extracting frustum point clouds from
RGB-D data <b> frustum proposal generation </b>. 

![Coordinate systems for point cloud](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-10.png?raw=true)


## 3.2.2. 3D Instance Segmentation

Given a 2D image region (and its corresponding 3D frustum), several methods might be used to obtain 3D location of the object: One straightforward solution is to directly regress 3D object locations (e.g., by 3D bounding
box) from a depth map using 2D CNNs. However, this
problem is not easy as occluding objects and background
clutter is common in natural scenes (as in Fig. 3), which
may severely distract the 3D localization task. Because objects are naturally separated in physical space, segmentation
in 3D point cloud is much more natural and easier than that
in images where pixels from distant objects can be near-by
to each other. Having observed this fact, we propose to segment instances in 3D point cloud instead of in 2D image or
depth map. Similar to Mask-RCNN, which achieves
instance segmentation by binary classification of pixels in
image regions, we realize 3D instance segmentation using a
PointNet-based network on point clouds in frustums.

![Challenges for 3D detection in frustum point cloud](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-11.png?raw=true)

Based on 3D instance segmentation, we are able to
achieve residual based 3D localization. That is, rather than
regressing the absolute 3D location of the object whose offset from the sensor may vary in large ranges (e.g. from 5m
to beyond 50m in KITTI data), we predict the 3D bounding
box center in a local coordinate system – 3D mask coordinates as shown in Fig. 4 (c).

<b> 3D Instance Segmentation PointNet </b>

The network takes
a point cloud in frustum and predicts a probability score for
each point that indicates how likely the point belongs to the
object of interest. Note that each frustum contains exactly
one object of interest. Here those “other” points could be
points of non-relevant areas (such as ground, vegetation) or
other instances that occlude or are behind the object of interest. Similar to the case in 2D instance segmentation, depending on the position of the frustum, object points in one
frustum may become cluttered or occlude points in another.
Therefore, our segmentation PointNet is learning the occlusion and clutter patterns as well as recognizing the geometry
for the object of a certain category.

In a multi-class detection case, we also leverage the semantics from a 2D detector for better instance segmentation. For example, if we know the object of interest is
a pedestrian, then the segmentation network can use this
prior to find geometries that look like a person. Specifically, in our architecture we encode the semantic category
as a one-hot class vector (k dimensional for the pre-defined
k categories) and concatenate the one-hot vector to the intermediate point cloud features.

After 3D instance segmentation, points that are classified
as the object of interest are extracted (“masking”).

![Frustum PointNets for 3D object detection](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-9.png?raw=true)

Having obtained these segmented object points, we further
normalize its coordinates to boost the translational invariance of the algorithm, following the same rationale as in
the frustum proposal step. In our implementation, we transform the point cloud into a local coordinate by subtracting
XYZ values by its centroid. This is illustrated in Fig. 4 (c).
Note that we intentionally do not scale the point cloud, because the bounding sphere size of a partial point cloud can
be greatly affected by viewpoints and the real size of the
point cloud helps the box size estimation.

## 3.2.3. 3D Amodal Box Estimation

Given the segmented object points (in 3D mask coordinate), this module estimates the object’s amodal oriented
3D bounding box by using a box regression PointNet together with a preprocessing transformer network.

- <b> Learning-based 3D Alignment by T-Net: </b> Even though
we have aligned segmented object points according to their
centroid position, we find that the origin of the mask coordinate frame (Fig. 4 (c)) may still be quite far from the amodal
box center. We therefore propose to use a light-weight regression PointNet (T-Net) to estimate the true center of the
complete object and then transform the coordinate such that
the predicted center becomes the origin (Fig. 4 (d)).

- <b> Amodal 3D Box Estimation PointNet: </b> The box estimation network predicts amodal bounding boxes (for entire object even if part of it is unseen) for objects given an object point cloud in 3D object coordinate (Fig. 4 (d)). The
network architecture is similar to that for object classification, however the output is no longer object class
scores but parameters for a 3D bounding box.

As stated in Sec. 3, we parameterize a 3D bounding box
by its center $(c_x, c_y, c_z)$, size $(h, w, l)$ and heading angle
$θ$ (along up-axis). We take a <i>“residual”</i> approach for box
center estimation. The center residual predicted by the box
estimation network is combined with the previous center
residual from the T-Net and the masked points’ centroid to
recover an absolute center (Eq. 1). For box size and heading
angle, we follow previous works and use a hybrid
of classification and regression formulations. Specifically
we pre-define $N S$ size templates and $N H$ equally split angle bins. Our model will both classify size/heading ($N S$
scores for size, $N H$ scores for heading) to those pre-defined
categories as well as predict residual numbers for each category ($3×NS$ residual dimensions for height, width, length,
NH residual angles for heading). In the end the net outputs
$3 + 4 × NS + 2 × NH$ numbers in total.

![Frustum PointNets for 3D object detection](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-12.png?raw=true)


## 3.3. Training with Multi-task Losses

We simultaneously optimize the three nets involved (3D
instance segmentation PointNet, T-Net and amodal box estimation PointNet) with multi-task losses (as in Eq. 2).
$L_{c1−reg}$ is for T-Net and $L_{c2−reg}$ is for center regression
of box estimation net. $L_{h−cls}$ and $L_{h−reg}$ are losses for
heading angle prediction while $L_{s−cls}$ and $L_{s−reg}$ are for
box size. Softmax is used for all classification tasks and
smooth-$l_1$ (huber) loss is used for all regression cases.


![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-13.png?raw=true)

<b> Corner Loss for Joint Optimization of Box Parameters: </b>
While our 3D bounding box parameterization is compact
and complete, learning is not optimized for final 3D box accuracy – center, size and heading have separate loss terms.
Imagine cases where center and size are accurately predicted but heading angle is off – the 3D IoU with ground
truth box will then be dominated by the angle error. Ideally all three terms (center,size,heading) should be jointly
optimized for best 3D box estimation (under IoU metric).
To resolve this problem we propose a novel regularization
loss, the corner loss:

![img](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-3-14.png?raw=true)


In essence, the corner loss is the sum of the distances
between the eight corners of a predicted box and a ground
truth box. Since corner positions are jointly determined by
center, size and heading, the corner loss is able to regularize
the multi-task training for those parameters.

To compute the corner loss, we firstly construct $NS × NH$ “anchor” boxes from all size templates and heading
angle bins. The anchor boxes are then translated to the estimated box center. We denote the anchor box corners as
$P^{ij}_k$, where $i$, $j$, $k$ are indices for the size class, heading
class, and (predefined) corner order, respectively. To avoid
large penalty from flipped heading estimation, we further
compute distances to corners ($P^{∗∗}_k$) from the flipped ground
truth box and use the minimum of the original and flipped
cases. $δ_{ij}$ , which is one for the ground truth size/heading
class and zero else wise, is a two-dimensional mask used to
select the distance term we care about.

## 3.4. Experiments

- [Origin Code](https://github.com/charlesq34/frustum-pointnets)





