---
title: Paper note 5 - SECOND - Sparsely Embedded Convolutional Detection
# author:
#   name: Life Zero
#   link: https://github.com/lacie-life
date:  2023-09-13 11:11:14 +0700
categories: [Computer Vision]
tags: [Paper]
img_path: /assets/img/post_assest/pvo/
render_with_liquid: false
---

[SECOND - Sparsely Embedded Convolutional Detection](https://www.mdpi.com/1424-8220/18/10/3337)

In this paper, Authors present a novel approach
called SECOND (<b>S</b>parsely <b>E</b>mbedded <b>CON</b>volutional <b>D</b>etection), which addresses these challenges
in 3D convolution-based detection by <b> maximizing the use of the rich 3D information present in
point cloud data </b>. This method incorporates several improvements to the existing convolutional
network architecture. <b> Spatially sparse convolutional networks are introduced for LiDAR-based
detection and are used to extract information from the z-axis before the 3D data are downsampled to
something akin to 2D image data </b>.

Another advantage of using point cloud data is that it is very <b> easy to scale, rotate and move
objects by applying direct transformations </b> to specified points on those objects. SECOND incorporates
a novel form of data augmentation based on this capability. A ground-truth database is generated
that contains the attributes of objects and the associated point cloud data. Objects sampled from this
database are then introduced into the point clouds during training. This approach can greatly increase
the convergence speed and the final performance of our network.

In addition to the above, we also <b> introduce a novel angle loss regression approach to solve the
problem of the large loss generated when the difference in orientation between the ground truth
and the prediction </b> is equal to $π$, which yields a bounding box identical to the true bounding box.
The performance of this angle regression approach surpasses that of any current method we know
about, including the orientation vector regression function available in AVOD. We also introduce
an auxiliary direction classifier to recognize the directions of objects.

# 1. SECOND Detector

## 1.1. Network Architecture

The proposed SECOND detector, depicted in Figure 1, consists of three components:

(1) A voxelwise feature extractor; 

(2) A sparse convolutional middle layer; 

(3) An RPN.

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-1.png?raw=true)

### 1.1.1. Point Cloud Grouping

Based on VoxelNet to obtain a voxel representation of the point cloud data. We first preallocate buffers based on the specified limit on the number of voxels;
then, we iterate over the point cloud and assign the points to their associated voxels, and we save the voxel coordinates and the number of points per voxel. We check the existence of the voxels based
on a hash table during the iterative process. If the voxel related to a point does not yet exist, we set
the corresponding value in the hash table; otherwise, we increment the number of voxels by one.
The iterative process will stop once the number of voxels reaches the specified limit. Finally, we obtain
all voxels, their coordinates and the number of points per voxel for the actual number of voxels. 

### 1.1.2. Voxelwise Feature Extractor

We use a voxel feature encoding (VFE) layer, to extract voxelwise features.
A VFE layer takes all points in the same voxel as input and uses a fully connected network (FCN)
consisting of a linear layer, a batch normalization (BatchNorm) layer and a rectified linear unit (ReLU)
layer to extract pointwise features. Then, it uses elementwise max pooling to obtain the locally
aggregated features for each voxel. Finally, it tiles the obtained features and concatenates these
tiled features and the pointwise features together. We use $VFE(c_{out})$ to denote a VFE layer that
transforms the input features into $c_{out}$-dimensional output features. Similarly, $FCN(c_{out})$ denotes
a Linear-BatchNorm-ReLU layer that transforms the input features into $c_{out}$-dimensional output
features. As a whole, the voxelwise feature extractor consists of several VFE layers and an FCN layer.

![Voxel feature encoding layer](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-4-2.png?raw=true)

### 1.1.3. Sparse Convolutional Middle Extractor

<b> Sparse Convolution Algorithm </b>

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-2.png?raw=true)


<b> Rule Generation Algorithm </b>

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-3.png?raw=true)


<b> Sparse Convolutional Middle Extractor </b>

Our middle extractor is used to learn information about the z-axis and convert the sparse 3D
data into a 2D BEV image. Figure 3 shows the structure of the middle extractor. It consists of two
phases of sparse convolution. Each phase contains several submanifold convolutional layers and one
normal sparse convolution to perform downsampling in the z-axis. After the z-dimensionality has been
downsampled to one or two, the sparse data are converted into dense feature maps. Then, the data are
simply reshaped into image-like 2D data.

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-4.png?raw=true)

### 1.1.4. Region Proposal Network

RPNs have recently begun to be used in many detection frameworks. In this work, we use
a single shot multibox detector (SSD)-like architecture to construct an RPN architecture. The input
to the RPN consists of the feature maps from the sparse convolutional middle extractor. The RPN
architecture is composed of three stages. Each stage starts with a downsampled convolutional layer,
which is followed by several convolutional layers. After each convolutional layer, BatchNorm and
ReLU layers are applied. We then upsample the output of each stage to a feature map of the same size
and concatenate these feature maps into one feature map. Finally, three 1 × 1 convolutions are applied
for the prediction of class, regression offsets and direction.

## 1.2. Training and Inference

### 1.2.1. Loss Function

<b> Sine-Error Loss for Angle Regression </b>

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-5.png?raw=true)

where the subscript $p$ indicates the predicted value. This approach to angle loss has two advantages:
(1) it solves the adversarial example problem between orientations of 0 and $π$, and (2) it naturally
models the IoU against the angle offset function. To address the issue that this loss treats boxes with
opposite directions as being the same, we have added a simple direction classifier to the output of the
RPN. This direction classifier uses a softmax loss function. We use the following approach to generate
the direction classifier target: if the yaw rotation around the z-axis of the ground truth is higher than
zero, the result is positive; otherwise, it is negative.

<b> Focal Loss for Classification </b>

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-6.png?raw=true)

<b> Total Training Loss </b>

By combining the losses discussed above, we can obtain the final form of the multitask loss
as follows:

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-7.png?raw=true)

### 1.2.2. Data Augmentation


<b> Sample Ground Truths from the Database </b>

The major problem we encountered during training was the existence of too few ground truths,
which significantly limited the convergence speed and final performance of the network. To solve this
problem, we introduced a data augmentation approach. First, we generated a database containing
the labels of all ground truths and their associated point cloud data (points inside the 3D bounding
boxes of the ground truths) from the training dataset. Then, during training, we randomly selected
several ground truths from this database and introduced them into the current training point cloud
via concatenation. Using this approach, we could greatly increase the number of ground truths per
point cloud and simulate objects existing in different environments. To avoid physically impossible
outcomes, we performed a collision test after sampling the ground truths and removed any sampled
objects that collided with other objects.


<b> Object Noise </b>

To consider noise, we followed the same approach used in VoxelNet, in which each ground
truth and its point cloud are independently and randomly transformed, instead of transforming
all point clouds with the same parameters. Specifically, we used random rotations sampled from
a uniform distribution $∆θ ∈ [−π/2, π/2]$ and random linear transformations sampled from a Gaussian
distribution with a mean of zero and a standard deviation of 1.0.

<b> Global Rotation and Scaling </b>

We applied global scaling and rotation to the whole point cloud and to all ground-truth boxes.
The scaling noise was drawn from the uniform distribution [0.95, 1.05], and $[−π/4, π/4]$ was used for
the global rotation noise.

### 1.2.3. Network Details

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-9.png?raw=true)

![SECOND detector](https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/paper-note-5-8.png?raw=true)


### Note

• We apply sparse convolution in LiDAR-based object detection, thereby greatly increasing the
speeds of training and inference.
• We propose an improved method of sparse convolution that allows it to run faster.
• We propose a novel angle loss regression approach that demonstrates better orientation regression
performance than other methods do.
• We introduce a novel data augmentation method for LiDAR-only learning problems that greatly
increases the convergence speed and performance.

# 2. Experiments

- [Code](https://github.com/traveller59/second.pytorch)





























