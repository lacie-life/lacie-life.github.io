<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="ML for 3D Geometry - Part 10" /><meta property="og:locale" content="en" /><meta name="description" content="Weak Supervision, n-shot Learning, Data Efficiency" /><meta property="og:description" content="Weak Supervision, n-shot Learning, Data Efficiency" /><link rel="canonical" href="https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/" /><meta property="og:url" content="https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/" /><meta property="og:site_name" content="Life Zero Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-12-02T11:11:14+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="ML for 3D Geometry - Part 10" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-12-03T15:19:36+07:00","datePublished":"2022-12-02T11:11:14+07:00","description":"Weak Supervision, n-shot Learning, Data Efficiency","headline":"ML for 3D Geometry - Part 10","mainEntityOfPage":{"@type":"WebPage","@id":"https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/"},"url":"https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/"}</script><title>ML for 3D Geometry - Part 10 | Life Zero Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Life Zero Blog"><meta name="application-name" content="Life Zero Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://live.staticflickr.com/7347/14119381583_6087a61c73_c_d.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Life Zero Blog</a></div><div class="site-subtitle font-italic">Life is hard but it's fair</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/lacie-life" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['00sao00ios00','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>ML for 3D Geometry - Part 10</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>ML for 3D Geometry - Part 10</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Life Zero </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Fri, Dec 2, 2022, 11:11 AM +0700" >Dec 2, 2022<i class="unloaded">2022-12-02T11:11:14+07:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sat, Dec 3, 2022, 3:19 PM +0700" >Dec 3, 2022<i class="unloaded">2022-12-03T15:19:36+07:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1130 words">6 min read</span></div></div><div class="post-content"><h1 id="weak-supervision-n-shot-learning-data-efficiency">Weak Supervision, n-shot Learning, Data Efficiency</h1><p>Broad motivation for this chapter: We want to use as little (annotated) data as possible, because data collection and data annotation are expensive.</p><h2 id="training-methods-and-required-amount-of-labeled-data">Training methods and required amount of labeled data</h2><ul><li><em>Supervised</em>: manually labeled data by expert annotators (expensive)<li><em>Unsupervised</em>: no annotations at all (learn structural patterns)<li><em>Semi-Supervision</em>: both a labeled and an unlabeled set of data<li><em>Weak Supervision</em>: lower quality labels that you can get without expert annotators<li><em>Self-Supervision</em>: Automatically generate supervision signal<li><em>Transfer Learning</em>: transfer pretrained models to your task</ul><p>Other hybrids exist: e.g. active learning -&gt; use annotations more efficiently (human in the loop)</p><h2 id="few-shot-learning">Few-shot learning</h2><p>See only few (or one) example per class.</p><h4 id="reconstruction-from-image-on-unseen-classes-zhang-et-al-18">Reconstruction from image on unseen classes [Zhang et al. ‘18]</h4><p>Problem usually: if you train on tables and chairs then want to reconstruct a bed, it might end up looking like a table. Goal in order to avoid: “the model should memorize as little as possible”</p><p>Input image -&gt; depth estimation -&gt; 2d spherical map (geometry outprojected into a sphere) -&gt; inpaint spherical map image to fill in missing geometry -&gt; backproject to 3d shape -&gt; refine to get a final 3d shape</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/reconstruction-on-unseen-classes.png?raw=true" alt="Fig.1" /></p><h4 id="learning-category-specific-mesh-reconstruction-kanazawa-et-al-18">Learning Category-Specific Mesh Reconstruction [Kanazawa et al. ‘18]</h4><p>Input: lots of annotated images of one specific class (e.g. birds), but no 3d information. Then for one specific image, reconstruct a textured mesh.</p><p>(didn’t get the details of the architecture)</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/bird-mesh-reconstructed.png?raw=true" alt="Fig.2" /></p><h4 id="few-shot-single-image-reconstruction-wallace-and-hariharan-19">Few-shot single image reconstruction [Wallace and Hariharan ‘19]</h4><p>Transform an input image to a 3d representation, but this time a <em>category prior</em> can be used (e.g. image of a sofa, and the category prior is the mean of all known sofas).</p><p>Trained on image/3d shape pairs. But at test time, novel categories are presented where there are only a few (say 25) examples.</p><p>Architecture: encode image with 2d convolutions, prior shape with 3d convolutions -&gt; concatenate (or sum?) the codes -&gt; decode back with transposed convoutions.</p><p>The limitation is of course that some categories have much diversity, and then the approach doesn’t work as well.</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/sofa-category-prior.png?raw=true" alt="Fig.3" /></p><h2 id="generalizing-accross-datasets">Generalizing accross datasets</h2><p>“Train in Germany, Test in the USA”. An issue e.g. in autonomous driving: if an object detector is trained in different circumstances than it used in. If applying such a model to the different test dataset blindly, there will be a performance gap that we want to avoid.</p><h4 id="wang-et-al-20">[Wang et al. ’20]</h4><p>Frequent issues found in self-driving car scenarios: misdetection (detect something where there isn’t anything), but even more mislocalization. The latter comes from different car sizes in e.g. Germany vs. the US, or accross different cities etc.</p><p>Solved by data normalization (<em>domain adaption</em>).</p><h2 id="approaches-with-lessno-supervision">Approaches with less/no supervision</h2><h4 id="discovery-of-latent-3d-keypoints-suwajanakorn-et-al-18">Discovery of Latent 3D Keypoints [Suwajanakorn et al. ’18]</h4><p>Given: multi-view image observations. Goal: find keypoints in 3D without having annotated training data. (Related: annotating 3D Keypoints is not well-defined anyway but very much subjective).</p><p>Use auxiliary task as weak supervisory signal: Aux. task is pose estimation from an image. Given are two views of which the ground-truth transformation between them is known . Then as supervision, they use that transforming the keypoints output for the first image should be close to the keypoints output for the second image</p><p>This matches what you want from a keypoint: Keypoints should be points that are identifiable from several views of the shape.</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/sofa-category-prior.png?raw=true" alt="Fig.4" /></p><h4 id="learning-the-depths-of-moving-people-li-et-al-19">Learning the Depths of Moving People [Li et al. ‘19]</h4><p>Input: Youtube videos of people standing still. Goal: estimate depth maps.</p><p>Using the “mannquin challenge” dataset (2000 videos): people try to stand still like mannequins, which satisfies the static scene assumption quite well. Via structure from motion and multiview stereo a depth image which serves as supervision signal ist estimated.</p><p>Goal at test time: also apply it to moving people, not just static people. To improve for this, also take into account a mask that shows where the humans are and a depth from flow estimation (obtained from having a video, not only a static image).</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/mannequin-challenge.png?raw=true" alt="Fig.5" /></p><h4 id="sg-nn-self-supervised-scan-completion-dai-et-al-20">SG-NN: Self-supervised scan completion [Dai et al. ‘20]</h4><p>Mentioned before [[09 - Reconstructing and Generating Scenes#SG-NN Self-supervised Scan Complete Dai et al ‘20|in Lecture 9]]. This also uses self-supervision to complete scans, while having only a dataset of incomplete scans, by learning how to go from less to more complete.</p><h5 id="pointcontrast-xie-et-al-20">PointContrast [Xie et al. ‘20]</h5><p>Goal: semantic understanding of point clouds. Pretrained on large unannotated dataset, fine-tuned on small dataset.</p><p>These idea come from similar ideas in 2D representation learning:</p><h5 id="excursion-2d-representation-learning">Excursion: 2D Representation Learning</h5><p>SimCLR [Chen et al. ‘20], MoCo [He et al. ‘20]. Use <em>contrastive loss function</em> to learn a representation where similar things are close, and dissimilar things are far away from each other. Compare with [[09 - Reconstructing and Generating Scenes#RetrievalFuse Siddiqui et al ’21|RetrievalFuse]], where a similar concept and loss were used.</p><p>The supervisory signal here is that we can generate more data samples we know to be similar by <em>data augmentation</em> (If you have an image of a cat and do some cropping and resizing, you know it’s still an image of a cat; so make sure that the model learns this).</p><h5 id="similar-ideas-used-in-pointcontrast">Similar ideas used in PointContrast</h5><p>In 3D, we can even use more augmentation techniques related to multi-view constraints. So pretrain the model to be able to recognize originally close points in 2 images of the same 3d scene as similar, and far away points as dissimilar.</p><h4 id="data-efficient-3d-scene-understanding-hou-et-al-21">Data Efficient 3D Scene understanding [Hou et al. ‘21]</h4><p>Extension of the previous PointContrast idea. Partition the scene into smaller regions, apply the contrastive loss in each region separately. This allows to get more performance out of using more sample points (while performance of the vanilla PointContrast approach saturates).</p><p>After the pretraining, train on limited number of fully-labeled scenes and on all scenes with limited point labeling budget (this would be suited to <em>active labeling</em>; not actually used here though).</p><h2 id="domain-adaption">Domain Adaption</h2><p>Scenario: Much labeled data in one domain, but much less in a second domain. One example: synthetic domain (object annotations for free) &lt;-&gt; real domain (annotations expensive).</p><p>Common tasks approached: semantic segmentation, object detection.</p><h3 id="excursion-domain-adaption-in-the-2d-domain">Excursion: Domain adaption in the 2D domain</h3><h4 id="cycada-cycle-consistent-adversarial-domain-adaption-hoffman-et-al-18">CyCADA: Cycle-Consistent Adversarial Domain Adaption [Hoffman et al. ‘18]</h4><p>Adapt between synthetic GTA images and real CityScapes images of cars on roads.</p><p>GAN: Generator should stylize the source image similar to the target image. Discrimnator tries to tell apart the images and also the extracted features. The source image and stylized source image should have semantically consistent features. Then goal: be able to use the same feature extractors from the source domain on the target domain.</p><h3 id="domain-adaption-in-3d">Domain Adaption in 3D</h3><p>No details here, but the same thing as for 2D applies: We have synthetic data that is labeled already, and want to use it for pretraining and then be able to apply the models to real data.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/theory/'>Theory</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine Learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=ML for 3D Geometry - Part 10 - Life Zero Blog&url=https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=ML for 3D Geometry - Part 10 - Life Zero Blog&u=https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=ML for 3D Geometry - Part 10 - Life Zero Blog&url=https://lacie-life.github.io/posts/ML-for-3D-Geometry-10/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/ZED2-and-ORB-SLAM3/">ZED2 with ORB-SLAM3 (Stereo-IMU mode) step-by-step</a><li><a href="/posts/Callback-Function-in-C++/">Designing Callbacks in C++</a><li><a href="/posts/ML-for-3D-Geometry-4/">ML for 3D Geometry - Part 4</a><li><a href="/posts/ML-for-3D-Geometry-5/">ML for 3D Geometry - Part 5</a><li><a href="/posts/ML-for-3D-Geometry-10/">ML for 3D Geometry - Part 10</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/ML-for-3D-Geometry-4/"><div class="card-body"> <span class="timeago small" >Dec 1, 2022<i class="unloaded">2022-12-01T11:11:14+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ML for 3D Geometry - Part 4</h3><div class="text-muted small"><p> Shape Generation Goal: be able to generate shapes automatically. Usecases: for example, allow amateurs to create quality 3D models, and professionals to reduce repetitive work. Also, complete 3D s...</p></div></div></a></div><div class="card"> <a href="/posts/ML-for-3D-Geometry-5/"><div class="card-body"> <span class="timeago small" >Dec 1, 2022<i class="unloaded">2022-12-01T11:11:14+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ML for 3D Geometry - Part 5</h3><div class="text-muted small"><p> Learning on Different 3D Representations Types of 3D Represenations Volumetric grids =&gt; 3D CNNs Multi-view Point clouds Meshes Volumetric Grids More efficient than dense grid? =&gt;...</p></div></div></a></div><div class="card"> <a href="/posts/ML-for-3D-Geometry-6/"><div class="card-body"> <span class="timeago small" >Dec 2, 2022<i class="unloaded">2022-12-02T11:11:14+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>ML for 3D Geometry - Part 6</h3><div class="text-muted small"><p> Semantic Scene Understanding - Semantic Segmentation 3D Semantic segmentation: segment scene (e.g. room) in semantic parts (e.g. for each part: to which furniture type it belongs) Popular Benchma...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/ML-for-3D-Geometry-5/" class="btn btn-outline-primary" prompt="Older"><p>ML for 3D Geometry - Part 5</p></a> <a href="/posts/ML-for-3D-Geometry-6/" class="btn btn-outline-primary" prompt="Newer"><p>ML for 3D Geometry - Part 6</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Life Zero</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://lacie-life.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
