<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art" /><meta property="og:locale" content="en" /><meta name="description" content="Machine Learning is Fun!: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art" /><meta property="og:description" content="Machine Learning is Fun!: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art" /><link rel="canonical" href="https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/" /><meta property="og:url" content="https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/" /><meta property="og:site_name" content="Life Zero Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-27T11:11:11+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-02-01T19:13:35+07:00","datePublished":"2022-01-27T11:11:11+07:00","description":"Machine Learning is Fun!: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art","headline":"Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art","mainEntityOfPage":{"@type":"WebPage","@id":"https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/"},"url":"https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/"}</script><title>Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art | Life Zero Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Life Zero Blog"><meta name="application-name" content="Life Zero Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://live.staticflickr.com/7347/14119381583_6087a61c73_c_d.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Life Zero Blog</a></div><div class="site-subtitle font-italic">Life is hard but it's fair</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/lacie-life" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['00sao00ios00','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Life Zero </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Jan 27, 2022, 11:11 AM +0700" >Jan 27, 2022<i class="unloaded">2022-01-27T11:11:11+07:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Feb 1, 2022, 7:13 PM +0700" >Feb 1, 2022<i class="unloaded">2022-02-01T19:13:35+07:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2291 words">12 min read</span></div></div><div class="post-content"><h1 id="machine-learning-is-fun-abusing-generative-adversarial-networks-to-make-8-bit-pixel-art">Machine Learning is Fun!: Abusing Generative Adversarial Networks to Make 8-bit Pixel Art</h1><p>Generative models allow a computer to create data — like photos, movies or music — by itself.</p><p>A little over a year ago, Alec Radford (building on the work of Ian Goodfellow) published a paper that changed how everyone thought about building generative models with machine learning. The new system is called Deep Convolutional Generative Adversarial Networks (or DCGANs for short).</p><p>DCGANs are able to hallucinate original photo-realistic pictures by using a clever combination of two deep neural networks that compete with each other. All of these pictures of bedrooms were dreamt up by a DCGAN:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/c1c17bb3-187c-4771-8a49-d96826f5cc98.png" alt="Fig.1" /></p><p>AI researchers care about generative models because they seem to be a stepping stone towards building AI systems that can consume raw data from the world and automatically build understanding from it.</p><p>But let’s use generative models to do something a bit more silly — make artwork for 8-bit video games!</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/7bcb99ef-3a82-4c50-91a4-1963a2930494.png" alt="Fig.2" /></p><h2 id="the-goal-of-generative-models">The goal of Generative Models</h2><p>So why exactly are AI researchers building complex systems to generate slightly wonky-looking pictures of bedrooms?</p><p>The idea is that if you can generate pictures of something, you must have an understanding of it.</p><p>Look at this picture:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/b185e913-53a2-4a27-8eee-22b2a110ab80.jpeg" alt="Fig.3" /></p><p>You instantly know this is a picture of a dog — a furry thing with four legs and a tail. But to a computer, the picture is just a grid of numbers representing the color of each pixel. The computer has no understanding that the picture represents a concept.</p><p>But now imagine that we showed a computer thousands of pictures of dogs and after seeing those pictures, the computer was able to generate new pictures of dogs on its own — including different dog breeds and pictures from different angles. Maybe we could even ask it for certain types of pictures, like “a side view of a beagle”.</p><p>If the computer was able to do this and the pictures it produced had the right number of legs, tails, and ears, it would prove that the computer knows what parts go into making up a “dog” even though no one told it explicitly. So in a sense, a good generative model is proof of basic understanding — at least on a toddler-level.</p><p>That’s why researchers are so excited about building generative models. They seem to be a way to train computers to understand concepts without being explicitly taught the meaning of those concepts. That’s a big step over current systems that can only learn from training data that has been painstakingly pre-labeled by humans.</p><p>But if all this research results in programs that generate pictures of dogs, how many years until we get the first computer-generated Dog-A-Day calendar as a side effect?</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/eea8e21f-2b22-4447-b7fe-3a2732952203.jpeg" alt="Fig.4" /></p><p>And if you can build a program that understands dogs, why not a program that understands anything else? What about a program that could generate an unlimited number of stock photos of people shaking hands? I’m sure someone would pay for that.</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/c469f27e-f18e-459d-884b-12b9904baf87.jpeg" alt="Fig.5" /></p><p>Ok, maybe a program that generates bad stock photos wouldn’t be that interesting. But given the rate of progress in generative models over just the past year, who knows where we’ll be in 5 or 10 years. What happens if someone invents a system to generate entire movies? Or music? Or video games?</p><p>If you look forward 20–30 years and squint, you can already imagine a world where entertainment could be 100% machine generated:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/b34f60b5-a672-4420-81dc-f31a650ccd7d.png" alt="Fig.6" /></p><p>The video game industry is the first area of entertainment to start seriously experimenting with using AI to generate raw content. Aside from the obvious Venn diagram overlap between computer gaming and machine learning engineers, there’s a huge cost incentive to invest in video game development automation given the $300+ million budgets of modern AAA video games.</p><p>We are still in the earliest days of machine-learning-based generative models and their practical uses are currently pretty narrow, but they are a lot of fun to play around with. Let’s see what we can do with one.</p><h2 id="how-dcgans-work">How DCGANs work</h2><p>To build a DCGAN, we create two deep neural networks. Then we make them fight against each other, endlessly attempting to out-do one another. In the process, they both become stronger. Let’s pretend that the first deep neural network is a brand new police officer who is being trained to spot counterfeit money. It’s job is to look at a picture and tell us if the picture contains real money.</p><p>Since we are looking for objects in pictures, we can use a standard Convolutional Neural Network for this job. If you aren’t familiar with ConvNets, you can read my earlier post. But the basic idea is that the neural network that takes in an image, processes it through several layers that recognize increasingly complex features in the image and then it outputs a single value—in this case, whether or not the image contains a picture of real money.</p><p>This first neural network is called the Discriminator:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/f62990f2-2f79-4e64-a63b-4fdd07754eb8.png" alt="Fig.7" /></p><p>Now let’s pretend the second neural network is a brand new counterfeiter who is just learning how to create fake money. For this second neural network, we’ll reverse the layers in a normal ConvNet so that everything runs backwards. So instead of taking in a picture and outputting a value, it takes in a list of values and outputs a picture.</p><p>This second neural network is called the Generator:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/6a811320-f7d4-4354-bec0-cb8138c80a26.png" alt="Fig.8" /></p><p>So now we have a police officer (the Discriminator) looking for fake money and a counterfeiter (the Generator) that’s printing fake money. Let’s make them battle!</p><p>In the first round, the Generator will create pathetic forgeries that barely resemble money at all because it knows absolutely nothing about what money is supposed to look like:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/7946ef18-807f-4ad3-aaa5-8332a9caa1fa.png" alt="Fig.9" /></p><p>But right now the Discriminator is equally terrible at it’s job of recognizing money, so it won’t know the difference:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/d075debf-61bd-4467-9229-5f0507abd0bf.png" alt="Fig.10" /></p><p>At this point, we step in and tell the Discriminator that this dollar bill is actually fake. Then we show it a real dollar bill and ask it how it looks different from the fake one. The Discriminator looks for a new detail to help it separate the real one from the fake one.</p><p>For example, the Discriminator might notice that real money has a picture of a person on it and the fake money doesn’t. Using this knowledge, the Discriminator learns how to tell the fake from the real one. It gets a tiny bit better at its job:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/74415195-946c-4d02-87b8-fa4b3caae4f3.png" alt="Fig.11" /></p><p>Now we start Round 2. We tell the Generator that it’s money images are suddenly getting rejected as fake so it needs to step up it’s game. We also tell it that the Discriminator is now looking for faces, so the best way to confuse the Discriminator is to put a face on the bill:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/fc1b6752-43ea-4c67-94d0-ecda3f99f980.png" alt="Fig.12" /></p><p>And the fake bills are being accepted as valid again! So now the Discriminator has to look again at the real dollar and find a new way to tell it apart from the fake one.</p><p>This back-and-forth game between the Generator and the Discriminator continues thousands of times until both networks are experts. Eventually the Generator is producing near-perfect counterfeits and the Discriminator has turned into a Master Detective looking for the slightest mistakes.</p><p>At the point when both networks are sufficiently trained so that humans are impressed by the fake images, we can use the fake images for whatever purpose we want.</p><h2 id="applying-this-to-video-games">Applying this to Video Games</h2><p>So now that we know how DCGANs work, let’s see if we can use one to generate new artwork for 1980s-style video games.</p><p>Let’s build a DCGAN that tries to produce screenshots of imaginary video games for the Nintendo Entertainment System (or NES) based on screenshots of real games:</p><p>The idea is that if we can generate convincing screenshots of imaginary video games, we could copy and paste bits of art from those screenshots and use it in our own retro-style video game. Since the generated video games never existed, it wouldn’t even be stealing (Maybe… more on this later).</p><p>Video game art in those days was very simple. Since the NES had such a small amount of memory (the games used way less memory than this article takes up!), programmers had to use lots of tricks to fit the game art into memory. To maximize the limited space, games used tile-based graphics where each screen in the game is made up of just a few (usually 16x16 pixel) repeated graphical tiles.</p><p>For example, the starting screen of ‘The Legend of Zelda’ is made up of only 8 unique tiles:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/65ceaaf3-46ae-47e0-bbdf-35c2e6926640.png" alt="Fig.13" /></p><p>Here are the tiles for entire ‘The Legend of Zelda’ game map:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/4fa35458-58ef-4a4c-b130-bc703db7ba96.png" alt="Fig.14" /></p><p>Our goal is to create a similar tile sheet for our game. Because of that, we don’t really care if the game screenshots we generate look completely realistic. Instead, we’re just looking for the shapes and patterns that we can use as 16 x 16 tiles in our game — things like stones, water, bridges, etc. Then we can use those tiles to build our own 8-bit-style video game levels.</p><h3 id="getting-data">Getting Data</h3><p>To train our system, we need lots of data. Luckily there are over 700 games for the NES that we can pull from.</p><p>I used wget to download all the NES game screenshots on The Video Game Museum website (sorry for scraping your site!). After a few minutes of downloading, I had a little over 10,000 screenshots of hundreds of NES games:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/f2512916-ab2a-472e-a99f-189034f07356.png" alt="Fig.15" /></p><p>Right now, DCGANs only work on pretty small images — 256 pixels square or so. But the entire screen resolution of the NES was only 256 pixels by 224 pixels, so that’s not a problem. To make things simple, I cropped each NES screenshot to 224 pixels square.</p><h3 id="setting-up-the-dcgan">Setting up the DCGAN</h3><p>There are several open-source implementations of DCGANs on github that you can try out. I used Taehoon Kim’s Tensorflow implementation. Since DCGANs are unsupervised, all you have to do is put the data in a folder, tweak the basic parameters, start it training and then wait to see what results you get.</p><p>Here’s what a sample of the original training data looks like:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/335aba49-082b-412f-9b0e-46c4d171f447.png" alt="Fig.16" /></p><p>Now training begins. At first, the output from the Generator is pure noise. But it slowly start to take shape as the Generator learns to do a better job:</p><p><img data-proofer-ignore data-src="https://miro.medium.com/max/700/1*zBFbVjBPHBqnwZfuCnYclg.gif" alt="Fig.17" /></p><p>After several more training rounds, the images start to resemble nightmare-ish versions of classic Nintendo games:</p><p><img data-proofer-ignore data-src="https://miro.medium.com/max/700/1*mcvGlV4iJSvrT33Ek5lEsQ.gif" alt="Fig.18" /></p><p>As training continues further, we start to see the bricks and blocks we are hoping to find. You can also see screen elements like life bars and even some text:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/e7d2533c-d448-4792-a491-db152c4b8461.png" alt="Fig.19" /></p><p>This is where things get complicated. How do we know the computer is creating brand new art and not just regurgitating art directly from the training images? In two of these images, you can clearly see the menu bar from Super Mario Bros. 3 and the header bar and bricks from the original Super Mario Bros.</p><p>Regurgitating training data is definitely something that can happen. By using a large training data set and not training too long, we can try to reduce the chance that this happens. But it’s a thorny issue and research on it continues.</p><p>Since I’m just going for aesthetics, I tweaked the model until it produced art that looked original to me. But I can’t prove that the new art is totally original except by searching the training data for similar art and verifying that there isn’t any.</p><p>With a few hours of training, the generated images contained 16 x 16 tiles that looked nice to me. I was looking for some variations on a basic stone block, brick patterns, water patterns, bushes, and some general “spooky-looking” background atmosphere tiles.</p><p>Next I need to pre-process the generated images to the make sure they only used the 64 colors that are available on the NES:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/90b8ea27-03bb-431b-aa68-d617c50d6043.png" alt="Fig.20" /></p><p>Then I’ll open up the 64-color images in the Tiled Map Editor. From there, I can easily grab the 16 x 16 tiles that match the aesthetic I want:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/b4ea9b8f-f20a-4533-8a36-168128323641.png" alt="Fig.21" /></p><p>Then inside of Tiled Map Editor, I’ll arrange those 16 x 16 tiles into a simple level layout reminiscent of the NES game ‘Castlevania’:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/2790f355-6be3-4b9c-8485-7ca1329797c5.png" alt="Fig.22" /></p><p>I think that looks pretty good! Keep in mind I didn’t touch a single pixel with an image editor. Every tile came straight out of the DCGAN model.</p><p>Next, let’s throw in the main character and some enemies from ‘Castlevania’ so we can see what this level would look like in action:</p><p><img data-proofer-ignore data-src="https://miro.medium.com/max/1000/1*XZK6GtAMA3CY-wrnzS04TQ.png" alt="Fig.23" /></p><p>To get the full effect, let’s see what the level would look like inside the game with the menu elements added:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/49946d28-e656-4b49-824e-afed825f23a9.png" alt="Fig.24" /></p><p>I think that looks like the NES games that I remember! I’m not claiming it’s the best NES art ever created, but it’s certainly not the worst:</p><p><img data-proofer-ignore data-src="https://miro.medium.com/max/256/1*wl-COd1eOmcJh84IVXgKCA.png" alt="Fig.25" /></p><h2 id="is-that-it">Is that it?</h2><p>I get really excited about generative models like this. The idea of one day cranking out endless artwork with computers is fascinating to me. But when I talk to other people about this stuff, sometimes the response is “Is that it? That’s so basic.”</p><p>There’s certainly a lot of hype around generative models right now. GANs are already being called the future of AI despite being notoriously hard to train and limited to generating tiny images. In fact, the very best models can currently only generate postage-stamp-sized pictures of mutant dogs:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/56f055be-0cd1-498a-a104-64b7bf9cf359.png" alt="Fig.26" /></p><p>But a couple of years ago, we couldn’t do anything close to that. We were pretty excited by generated pictures that looked like this:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/1ba7a39e-9e90-4e62-b689-d8c5ff35799a.png" alt="Fig.27" /></p><p>And the technology is improving every single day. Here’s a random paper that came out this week that uses GANs to age the faces of people:</p><p><img data-proofer-ignore data-src="https://images.viblo.asia/8c1a36b3-00dd-4a5f-b363-004384debcd4.png" alt="Fig.28" /></p><p>If things keep improving at this pace, it won’t be too long before generative models are a mainstream tool helping us create. It’s a great time to start experimenting!</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/machine-learning/'>Machine Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/tutorial/" class="post-tag no-text-decoration" >tutorial</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art - Life Zero Blog&url=https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art - Life Zero Blog&u=https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Machine Learning is Fun! - Abusing Generative Adversarial Networks to Make 8-bit Pixel Art - Life Zero Blog&url=https://lacie-life.github.io/posts/Machine-Learning-Simple-View-Abusing-Generative-Adversarial-Networls-to-Make-8bit-Pixel-Art/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/ZED2-and-ORB-SLAM3/">ZED2 with ORB-SLAM3 (Stereo-IMU mode) step-by-step</a><li><a href="/posts/Callback-Function-in-C++/">Designing Callbacks in C++</a><li><a href="/posts/ML-for-3D-Geometry-4/">ML for 3D Geometry - Part 4</a><li><a href="/posts/ML-for-3D-Geometry-5/">ML for 3D Geometry - Part 5</a><li><a href="/posts/ML-for-3D-Geometry-10/">ML for 3D Geometry - Part 10</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Part-1-Linux-Directory-Structure/"><div class="card-body"> <span class="timeago small" >Sep 13, 2021<i class="unloaded">2021-09-13T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Linux Directory Structure</h3><div class="text-muted small"><p> Linux Directory Structure / – root Every single file and directory starts from the root directory. Only root user has write privilege under this directory. Please note that /root is root us...</p></div></div></a></div><div class="card"> <a href="/posts/Part-2-The-Shell/"><div class="card-body"> <span class="timeago small" >Sep 14, 2021<i class="unloaded">2021-09-14T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The Shell Theory</h3><div class="text-muted small"><p> The Shell Theory What is The Shell ? A Shell provides you with an interface to the Unix system. It gathers input from you and executes programs based on that input. When a program finishes ex...</p></div></div></a></div><div class="card"> <a href="/posts/Part-3-Basic-Linux-Command/"><div class="card-body"> <span class="timeago small" >Sep 15, 2021<i class="unloaded">2021-09-15T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Basic Linux Command</h3><div class="text-muted small"><p> Basic Linux Command ls - List Directories Content (in windows we call these as a folders) cd - Changes the current directories pwd - Displays the present working direct...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Machine-Learning-Simple-View-How-to-do-Speech-Recognition-with-Deep-Learning/" class="btn btn-outline-primary" prompt="Older"><p>Machine Learning is Fun! - How to do Speech Recognition with Deep Learning</p></a> <a href="/posts/Machine-Learning-Simple-View-How-to-Intentionally-Trick-Neural-Networks/" class="btn btn-outline-primary" prompt="Newer"><p>Machine Learning is Fun! - How to Intentionally Trick Neural Networks</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Life Zero</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://lacie-life.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
