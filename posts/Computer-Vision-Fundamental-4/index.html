<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Computer Vision Fundamental - [Part 4]" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 4 - Estimating Point Correspondence Goal: find point descriptors/characteristic image features to be able to identify keypoints accross several images from different views." /><meta property="og:description" content="Chapter 4 - Estimating Point Correspondence Goal: find point descriptors/characteristic image features to be able to identify keypoints accross several images from different views." /><link rel="canonical" href="https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/" /><meta property="og:url" content="https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/" /><meta property="og:site_name" content="Life Zero Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-13T11:11:11+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Computer Vision Fundamental - [Part 4]" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-06-17T23:58:37+07:00","datePublished":"2022-06-13T11:11:11+07:00","description":"Chapter 4 - Estimating Point Correspondence Goal: find point descriptors/characteristic image features to be able to identify keypoints accross several images from different views.","headline":"Computer Vision Fundamental - [Part 4]","mainEntityOfPage":{"@type":"WebPage","@id":"https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/"},"url":"https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/"}</script><title>Computer Vision Fundamental - [Part 4] | Life Zero Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Life Zero Blog"><meta name="application-name" content="Life Zero Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://live.staticflickr.com/7347/14119381583_6087a61c73_c_d.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Life Zero Blog</a></div><div class="site-subtitle font-italic">Life is hard but it's fair</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/lacie-life" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['00sao00ios00','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Computer Vision Fundamental - [Part 4]</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Computer Vision Fundamental - [Part 4]</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Life Zero </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Jun 13, 2022, 11:11 AM +0700" >Jun 13, 2022<i class="unloaded">2022-06-13T11:11:11+07:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Fri, Jun 17, 2022, 11:58 PM +0700" >Jun 17, 2022<i class="unloaded">2022-06-17T23:58:37+07:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1063 words">5 min read</span></div></div><div class="post-content"><h1 id="chapter-4---estimating-point-correspondence">Chapter 4 - Estimating Point Correspondence</h1><p>Goal: find point descriptors/characteristic image features to be able to identify keypoints accross several images from different views.</p><p>Things to consider:</p><ul><li>small vs. wide baseline case (small vs. large displacements)<ul><li>higher fps makes things easier!<li>small baseline: plausible corresponding points are likely to be close (only search there)</ul><li>textured reconstructions can look misleadingly good (egg with face projected looks a lot like a human head)<li>non-Lambertian materials (shiny: view point dependence of reflection)<li>non-rigid transformations (person bending their head)<li>partial occlusions (sunglasses) - a point may not have a correspondence</ul><h5 id="small-deformation-vs-wide-baseline">Small Deformation vs. Wide Baseline</h5><ul><li><em>small deformation case</em>: classical <em>optical flow estimation</em>.<ul><li>Lucas/Kanade method (1981) (find correspondences sparsely)<li>Horn/Schunk method (1981) (find correspondences densely)</ul><li><em>wide baseline stereo</em>: Select feature points and find an appropriate pairing of points</ul><p>Comment: with improving methods, increasingly large deformations can be handled by optical flow estimation (e.g. coarse-to-fine approaches)</p><h3 id="small-deformations-optical-flow">Small Deformations: Optical Flow</h3><p>The rigid transformation of a point $x_1$ in one image to $x_2$ in the other image is given by</p>\[x_2 = h(x_1) = \frac{1}{\lambda_2(X)}(R \lambda_1(X) x_1 + T)\]<h5 id="local-approximation-models-for-motion">Local approximation models for motion</h5><p>This can be approximated locally e.g. by a <em>translational model</em></p>\[h(x) = x + b\]<p>or an affine model</p>\[h(x) = Ax + b\]<p>The 2D affine model can also be written as</p>\[h(x) = x + u(x) = x + S(x) p = \begin{pmatrix} x &amp; y &amp; 1 &amp; &amp; &amp; \\ &amp; &amp; &amp; x &amp; y &amp; 1\end{pmatrix}(p_1, p_2, p_3, p_4, p_5, p_6)^\top\]<p>for some parameters $p_i$ depending on the rotation/translation.</p><p>Affine models include much more types of motion (divergent motions, rotations etc.)</p><h5 id="optical-flow-estimation">Optical Flow Estimation</h5><p>The <em>optical flow</em> refers to the part of the motion that can be seen in the image plane (i.e. the projection of the real motion onto the image plane).</p><ul><li><strong>Lucas-Kanade</strong>: sparse method (estimate motion field at certain points, under the assumption that the motion in a small neighborhood is <em>constant</em>)<li><strong>Horn-Schunck</strong>: dense method (estimate motion field at every pixel, under the assumption that the motion in a small neighborhood is <em>smooth</em>)</ul><p>Lucas-Kanade was prefered at the time the methods were published because it is simpler and already was realtime-capable in the 80’s. In more recent years, Horn-Schunck is becoming more popular (“now we have GPUs”).</p><h3 id="the-lucas-kanade-method">The Lucas-Kanade Method</h3><h4 id="some-assumptions-we-make">Some Assumptions we make</h4><ul><li><em>Brightness Constancy Assumption</em> (also optical flow constraint): every moving point has constant brightness. Formally, $I(x(t), t) = \text{const.} ~\forall t$.<ul><li>This is <em>almost never</em> fulfilled. But often approximately<li>the equivalent formulation \(\frac{d}{dt}I(x(t), t) = \nabla I^\top \frac{dx}{dt} + \frac{\partial I}{\partial t} = 0\)is also called the (differential) optical flow constraint</ul><li><em>Constant Motion in a Neighborhood Assumption</em>: the velocity of movement is constant in a neighborhood $W(x)$ of a point $x$: \(\nabla I(x', t)^\top v + \frac{\partial I}{\partial t}(x', t) = 0 \quad \forall x' \in W(x)\)</ul><h4 id="lukas-kanade-1981-formulation">Lukas-Kanade (1981) formulation</h4><p>Since the two assumptions are not exactly fulfilled usually, the method minimizes the least-squares error instead: \(E(v) = \int_{W(x)} \vert \nabla I(x', t)^\top v + I_t(x', t)\vert^2 dx'\) Comment: this would be done differently today (not quadratic e.g.). $E$ (cost function) is also called <em>energy</em>.</p><h5 id="solution">Solution</h5><p>We get $\frac{dE}{dv} = 2Mv + 2q = 0$, where $M = \int_{W(x)} \nabla I \nabla I^\top dx’$ and $q = \int_{W(x)} I_t \nabla I dx’$. If $M$ is invertible, the solution is $v = - M^{-1} q$.</p><h5 id="alternatives">Alternatives</h5><p>Affine motion: basically, same technique. The cost function becomes</p>\[E(v) = \int_{W(x)} \vert \nabla I(x', t)^\top S(x') p + I_t(x', t)\vert^2 dx'\]<p>and is minimized with the same technique as above.</p><h5 id="limitations-translational-version">Limitations (translational version)</h5><p><em>Aperture problem</em>: e.g. for constant intensity regions (where $\nabla I(x) = 0, I_t(x) = 0$ for all points). To get a unique solution $b$, the <em>structure tensor</em> $M(x)$ must be invertible:</p>\[M(x) = \int_{W(x)} \begin{pmatrix} I_x^2 &amp; I_x I_y \\ I_x I_y &amp; I_y^2 \end{pmatrix} \,dx'\]<p>If $M(x)$ is not invertible, but at least non-zero, at least the <em>normal motion</em> (motion in direction of the gradient) can be estimated.</p><h4 id="simple-feature-tracking-with-lucas-kanade">Simple Feature Tracking with Lucas-Kanade</h4><p>Assume: given $t$.</p><ul><li>for each $x \in \Omega$ compute the structure tensor $M(x)$<li>for the points $x$ where $\det M(x) \geq 0$ (above treshold), compute the local velocity as \(b(x, t) = -M(x)^{-1} \begin{pmatrix} \int I_x I_t dx' \\ \int I_y I_t d x' \end{pmatrix}\)<li>update points from $x$ to $x + b(x, t)$ and repeat for time $t + 1$.</ul><p>Important point: a translation-only model works in small window, but on a larger window, or with a longer movement, we need a better model (e.g. affine).</p><h3 id="robust-feature-point-extraction">Robust Feature Point Extraction</h3><p>Problem: unreliable to invert $M$ if it has a small determinant. Alternative: Förstner 1984, Harris &amp; Stephens 1988 - <em>Harris corner detector</em>.</p><ul><li>use alternative structure tensor to detect good points<ul><li>weight neighborhood by a Gaussian: \(M(x) = G_\sigma \nabla I \nabla I^\top = \int G_\sigma(x - x') \begin{pmatrix} I_x^2 &amp; I_x I_y \\ I_x I_y &amp; I_y^2 \end{pmatrix}(x') \,dx'\)<li>select points for which $\det(M) - \kappa \,\text{trace}(M)^2 &gt; \theta$</ul></ul><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/harris-foerstner-detector.png?raw=true" alt="Fig.3" /></p><h3 id="wide-baseline-matching">Wide Baseline Matching</h3><p>Problem: many points will have no correspondence in the second image. Wide baseline might be needed to counter <em>drift</em>, i.e. the accumulation of small errors (compute corredpondences again with larger distance in time).</p><ul><li>One needs to consider an affine model (translational is not good enough for wide baseline).<li>To be more robust to illumination changes (typically greater in wide baseline): replace L2 error function by <em>normalized cross correlation</em></ul><h5 id="normalized-cross-correlation">Normalized Cross Correlation</h5><p>The NCC for a given candidate transformation $h$ is \(NCC(h) = \frac{ \int_{W(x)} (I_1(x') - \bar{I}_1) (I_2(h(x')) - \bar{I}_2) \, dx'} { \sqrt{\int_{W(x)} (I_1(x') - \bar{I}_1)^2 \, dx' \int_{W(x)} (I_2(x') - \bar{I}_2)^2 \, dx'}}\)</p><p>where $\bar{I}_1, \bar{I}_2$ are average intensities of $W(x)$ ($\bar{I}_2$ depends on $h$). Subtracting averages leads to invariance wrt. additive intensity changes. Dividing by the intensity variances of the window leads to invariance to multiplicative changes.</p><p>Different interpretation: If we stack the normalized intensity values of a window in one vector, $v_i = \text{vec}(I_i - \bar{I}_i), i =1,2$, then $NCC(h) = \cos \angle (v_1, v_2)$.</p><h5 id="normalized-cross-correlation-for-affine-transformation">Normalized Cross Correlation for affine transformation</h5><p>Affine transformation $h(x) = Ax + d$: Find optimum $\arg\max_{A,d} NCC(A, d)$. Just insert the $h$ in the above formula to get the $NCC$. Efficiently finding optima is a challenge</p><h5 id="optical-flow-estimation-with-deep-neural-networks">Optical Flow Estimation with Deep Neural Networks</h5><p>Deep NNs can also be used for correspondence estimation and have become more popular in recent years.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/skill/'>Skill</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/theory/" class="post-tag no-text-decoration" >Theory</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Computer Vision Fundamental - [Part 4] - Life Zero Blog&url=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Computer Vision Fundamental - [Part 4] - Life Zero Blog&u=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Computer Vision Fundamental - [Part 4] - Life Zero Blog&url=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-4/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/ZED2-and-ORB-SLAM3/">ZED2 with ORB-SLAM3 (Stereo-IMU mode) step-by-step</a><li><a href="/posts/Callback-Function-in-C++/">Designing Callbacks in C++</a><li><a href="/posts/ML-for-3D-Geometry-4/">ML for 3D Geometry - Part 4</a><li><a href="/posts/ML-for-3D-Geometry-5/">ML for 3D Geometry - Part 5</a><li><a href="/posts/ML-for-3D-Geometry-10/">ML for 3D Geometry - Part 10</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Computer-Vision-Fundamental-1/"><div class="card-body"> <span class="timeago small" >Jun 10, 2022<i class="unloaded">2022-06-10T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 1]</h3><div class="text-muted small"><p> These notes are lecture notes on the Computer Vision II - Multiple View Geometry course held in the summer term 2021 by Prof. Daniel Cremers/Prof. Florian Bernard. Ref Chapter 1 - Mathematical Ba...</p></div></div></a></div><div class="card"> <a href="/posts/Computer-Vision-Fundamental-2/"><div class="card-body"> <span class="timeago small" >Jun 11, 2022<i class="unloaded">2022-06-11T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 2]</h3><div class="text-muted small"><p> Chapter 2 - Representing a Moving Scene Origins of 3D Reconstruction 3D reconstruction is a classical ill-posed problem, as its solutions are not unique (most extreme example: imagine a photograph...</p></div></div></a></div><div class="card"> <a href="/posts/Computer-Vision-Fundamental-3/"><div class="card-body"> <span class="timeago small" >Jun 12, 2022<i class="unloaded">2022-06-12T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 3]</h3><div class="text-muted small"><p> Chapter 3 - Perspective Projection Goal of MVG: invert the image formation process. One part of the formation process is the camera motion (last lecture). The second one is the projection of point...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Computer-Vision-Fundamental-3/" class="btn btn-outline-primary" prompt="Older"><p>Computer Vision Fundamental - [Part 3]</p></a> <a href="/posts/Computer-Vision-Fundamental-5/" class="btn btn-outline-primary" prompt="Newer"><p>Computer Vision Fundamental - [Part 5]</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Life Zero</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://lacie-life.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
