<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="Computer Vision Fundamental - [Part 5]" /><meta property="og:locale" content="en" /><meta name="description" content="Chapter 5 - Reconstruction from Two Views" /><meta property="og:description" content="Chapter 5 - Reconstruction from Two Views" /><link rel="canonical" href="https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/" /><meta property="og:url" content="https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/" /><meta property="og:site_name" content="Life Zero Blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-06-14T11:11:11+07:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Computer Vision Fundamental - [Part 5]" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-06-17T23:58:37+07:00","datePublished":"2022-06-14T11:11:11+07:00","description":"Chapter 5 - Reconstruction from Two Views","headline":"Computer Vision Fundamental - [Part 5]","mainEntityOfPage":{"@type":"WebPage","@id":"https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/"},"url":"https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/"}</script><title>Computer Vision Fundamental - [Part 5] | Life Zero Blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Life Zero Blog"><meta name="application-name" content="Life Zero Blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang=""><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://live.staticflickr.com/7347/14119381583_6087a61c73_c_d.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Life Zero Blog</a></div><div class="site-subtitle font-italic">Life is hard but it's fair</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/lacie-life" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['00sao00ios00','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Computer Vision Fundamental - [Part 5]</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"> <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Computer Vision Fundamental - [Part 5]</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Life Zero </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jun 14, 2022, 11:11 AM +0700" >Jun 14, 2022<i class="unloaded">2022-06-14T11:11:11+07:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Fri, Jun 17, 2022, 11:58 PM +0700" >Jun 17, 2022<i class="unloaded">2022-06-17T23:58:37+07:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1624 words">9 min read</span></div></div><div class="post-content"><h1 id="chapter-5---reconstruction-from-two-views">Chapter 5 - Reconstruction from Two Views</h1><h3 id="assumptions">Assumptions</h3><ul><li>two consecutive frames<li>given: set of corresponding points<li>static scene (no movements during camera motion)<li>intrinsic camera parameters are known + fixed<ul><li>e.g. focal lengths, radial distortion, …</ul></ul><h3 id="problem-formulation">Problem formulation</h3><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/twoView.png?raw=true" alt="Fig.1" /></p><p>two problems: estimate 3d coordinates of known points; estimate camera motion. Chicken-egg problem! Approach:</p><ol><li>estimate camera motion<li>reconstruct 3d coordinates (by <em>triangulation</em>)</ol><h3 id="epipolar-geometry">Epipolar Geometry</h3><p>$o_1, o_2$: Camera centers $P$ or $X$: 3d point that we know a correspondence of $x_1, x_2$: projections of $P$ on the two image planes $e_1, e_2$: <em>epipoles</em> (intersections of line $(o_1, o_2)$ with image planes) $l_1, l_2$: <em>epipolar lines</em>: line between $x_i$ and $e_i$.</p><p>Also: <em>epipolar plane</em> associated with the 3d point $X$ (?)</p><p>One property: line $(o_2, x_1)$ passes through $l_2$.</p><h3 id="parametric-formulation--projection-error-cost-function">Parametric formulation / projection error cost function</h3><p>Minimize <em>projection error</em> $E(R, T, X_1, \dots, X_N)$ with 6 params for camera motion $R, T$ and $3N$ (say 300) params for the 3d coordinates. \(E(R, T, X_1, \dots, X_N) = \sum_j ||x_1^j - \pi(X_j)||^2 + ||x_2^j - \pi(R, T, X_j)||^2\) (here we are in the 1-st camera coordinate system and rotate to get to the 2-nd camera coordinate system)</p><p>Hard to solve! Either</p><ol><li>stick with this cost function and use some advanced optimization techniques<li>re-formulate problem, get rid of 3d coordinates, effectively a different cost function =&gt; epipolar constraint, 8-point alg.</ol><h3 id="epipolar-constraint">Epipolar Constraint</h3><p>Projection $X \mapsto x_1$ is simply a projection with unknown depth $\lambda_1$: $\lambda_1 x_1 = X$. For the second camera, we get $\lambda_2 x_2 = RX + T$.</p><p>Next: $\lambda_2 x_2 = R(\lambda_1 x_1) + T$ Multiplying with $\hat{T}$ (removes $T$) gives $\lambda_2 \hat{T} x_2 = \lambda_1 \hat{T} R x_1$. The LHS is orthogonal to $x_2$: So projecting onto $x_2$, i.e. scalar product with $x_2$, yields</p>\[x_2^\top \hat{T} R x_1 = 0. \qquad\text{(Epipolar Constraint)}\]<p>Goal now: given enough epipolar constraints - one for each corresponding point pair - we’ll get an estimation for $R$ and $T$? $E = \hat{T} R$ is called the <em>essential matrix</em>.</p><h5 id="geometric-interpretation">Geometric interpretation</h5><p>the constraint states that the vectors $o_1X$, $o_2o_1$, $o_2X$ form a plane; i.e. $o_1X$, $o_2X$ intersect (obvious if phrased like this, but the constraint expresses it in terms of $x_1, x_2, R, T$)</p><p>In 2nd-frame coords, $Rx_1 \sim o_1 X$, $T \sim o_2o_1$, $x_2 ~ o_2X$. Then volume = $x_2^\top (T \times R x_1) = 0$.</p><h5 id="properties-of-e">Properties of E</h5><p>Essential space: $\mathcal{E} = { \hat{T} R | R \in SO(3), T \in \mathbb{R}^3 }$.</p><p><strong>Theorem:</strong> $0 \neq E \in \mathcal{E}$ iff SVD of E = $U \Sigma V^\top$, $\Sigma = \text{diag}(\sigma, \sigma, 0)$, $\sigma &gt; 0$, $U, V \in SO(3)$.</p><p><strong>Theorem:</strong> (Pose recovery from E) There are 2 relative poses $(R, T)$ corresponding to some $E \in \mathcal{E}$: \((\hat{T}_{1,2}, R_{1,2}) = \bigg(U R_Z \big(\pm \tfrac{\pi}{2}\big) \Sigma U^\top, U R_Z\big(\pm \tfrac{\pi}{2}\big)^\top V^\top\bigg)\)</p><p>Only one solution is sensible (has positive depth)</p><h3 id="basic-reconstruction-algorithm-essential-matrix">Basic Reconstruction Algorithm (Essential Matrix)</h3><ol><li>Recover essential matrix $E$ from epipolar constraints<li>Extract translation/rotation from $E$</ol><p>$E$ recovered from several constraints will not be an essential matrix in general. We can either</p><ul><li>project the matrix we recover to $\mathcal{E}$ or<li>optimize epipolar constraints in the essential space (more accurate, but more involved - nonlinear constrained opt.)</ul><p>Using the first approach, this leads to:</p><h3 id="the-8-point-algorithm">The 8-point algorithm</h3><p>Write epipolar constraints as scalar product: \(x_2^\top E x_1 = a^\top E^s = 0\) where $a = x_1 \otimes x_2$ and for $n$ point pairs: \(\chi E^s = 0, \chi = (a^1, \dots, a^n)^\top\)</p><p>So the flattened matrix $E$ lives in $\ker \chi$. For a unique solution (up to a scaling factor), we need $\text{rank}(\chi) = 8$. =&gt; exactly 8 point pairs.</p><p>The scaling factor can never be figured out without additional information (“scale freedom” of images). Possible: just fix translation to be 1.</p><p>Important: a degenerate case is if all points lie on a line or on a plane.</p><p>Another non-uniqueness: if E is a solution, so is -E; each E produces two solutions.</p><h5 id="projection-on-essential-space-mathcale">Projection on Essential Space $\mathcal{E}$</h5><p>Starting with arbitrary $F = U \text{diag}(\lambda_1, \lambda_2, \lambda_3) V^\top$, $\lambda_i$ descending, the essential matrix $E$ with minimum Frobenius distance from $F$ is \(E = U \text{~diag}(\tfrac{\lambda_1 + \lambda_2}{2}, \tfrac{\lambda_1 + \lambda_2}{2}, 0) V^\top.\) Even simpler: singular values (1, 1, 0), since scale doesn’t matter.</p><h5 id="8-point-algorithm-formulation">8-Point Algorithm Formulation</h5><ol><li>Approximate $E$<ol><li>Compute $\chi$<li><div class="table-wrapper"><table><tbody><tr><td>Find $E^s$ that minimizes $<td> <td>\chi E^s<td> <td>$: This is the ninth column of $V_\chi$ in the SVD $\chi = U_\chi \Sigma_\chi V_\chi^\top$.</table></div><li>Unstack $E^s$ into $E$.</ol><li>Project $E$ onto $\mathcal{E}$<ol><li>Compute SVD of $E$<li>Replace singular values with (1, 1, 0) (projection onto normalized essential space - this is ok since $E$ is only defined up to a scalar)</ol><li>Recover R, T from essential matrix<ol><li>$R = U R_Z^\top(\pm \tfrac{\pi}{2}) V^\top$<li>$\hat{T} = U R_Z(\pm \tfrac{\pi}{2}) \Sigma U^\top$ where $R_Z^\top(\pm \tfrac{\pi}{2}) = \pmatrix{&amp; \pm 1 &amp; \ \mp 1 &amp; &amp; \ &amp; &amp; 1}$ (watch out: transposed!)</ol></ol><h3 id="we-actually-only-need-5-points-kruppa-1913">We actually only need 5 points (Kruppa 1913)</h3><p>$E$ is actually a five-dimensional space. Example: with 7 points, $\ker \chi$ is (at least) 2-dim, spanned by $E_1, E_2$. Solve for $E$: find $E = E_1 + \alpha E_2$ s.t. $\det(E) = 0$ ($\det E = 0$ is one of the algebraic properties of $E$).</p><p>Kruppa 1913 proved: we only need five points to recover $(R, T)$; even less for degenerate (e.g. planar/circular) motion.</p><h3 id="selecting-solutions">Selecting solutions</h3><p>Only one of the four solutions gives positive depth values to all points.</p><h3 id="limitations--extensions">Limitations / Extensions</h3><p>If there is no translation, only rotation, $E = 0$. Then nothing can be recovered! Does not happen usually due to noise.</p><p>For “infinitesimal” view point changes: <em>continuous epipolar constraint</em>. Recovers linear and angular camera velocity instead of $(R, T)$.</p><p>Moving objects (independently): $(x_2^\top E_1 x_1) (x_2^T E_2 x_1) = 0$ for two essential matrices $E_1, E_2$ (“each part is either on the car or on the background”).</p><ul><li>can be solved with enough points (“polynomial factorization techniques”)</ul><h2 id="reconstructing-3d-structure-from-r-t">Reconstructing 3D Structure from R, T</h2><h3 id="structure-reconstruction">Structure Reconstruction</h3><p>After recovering R, T, we have for j-th point $X^j$: \(\lambda_2^j x_2^j = \lambda_1^j R x_1^j + \gamma T\) Scale params $\lambda_{1,2}^j, \gamma$ are unknown.</p><p>Eliminating $\lambda_2^j$ with $\widehat{x_2^j}$: \(\lambda_1^j \widehat{x_2^j} R x_1^j + \gamma \widehat{x_2^j} T = 0\)</p><p>Leads to linear system with $n+1$ variables: $M \vec{\lambda} \approx 0$, $\vec{\lambda} = (\lambda_1^1, …, \lambda_1^n, \gamma)^T \in \mathbb{R}^{n+1}$</p><div class="table-wrapper"><table><tbody><tr><td>Solve this via least-squares: solve $\min\limits_{<td> <td>\vec{\lambda}<td> <td>=1}<td> <td>M \vec{\lambda}<td> <td>^2$. A.k.a find the eigenvector to the smallest eigenvalue.</table></div><h3 id="issues-with-noisy-inputs">Issues with Noisy Inputs</h3><p>The 8-point algorithm is not robust against noise: small perturbations can lead to large errors in the reconstruction.</p><p>Underlying reason: small changes can change eigenvalue structure.</p><h3 id="more-robust-approaches">More robust approaches</h3><h5 id="bayesian-approach">Bayesian approach</h5><p>Maximum aposteriori estimate: \(\arg\max\limits_{x, R, T} P(x, R, T | \tilde{x}) = \arg\max\limits_{x, R, T} P(\tilde{x} | x, R, T) P(x, R, T)\)</p><p>Problem: hard to define distributions on $SO(3) \times \mathbb{S}^2$.</p><h5 id="constrained-optimization">Constrained optimization</h5><p>Minimize cost function: \(\phi(x, R, T) = \sum_{j=1}^n \sum_{i=1}^1 ||\tilde{x}_i^j - x_i^j||^2\) subject to constraints: $x_2^i{}^\top \hat{T} R x^j_1 = 0$, $x_1^i{}^\top e_3 = 1$, $x_2^j{}^\top e_3 = 1$</p><p>I.e. find points as close to the estimations as possible that fulfil the epipolar constraints.</p><h5 id="bundle-adjustment">Bundle Adjustment</h5><p>An “unconstrained” version, which has the unknown depth params $\lambda_i$,: \(\sum_{j=1}^n ||\tilde{x}^j_1 - \pi_1(X^j)||^2 + ||\tilde{x}^j_2 - \pi_2(X^j)||^2\) (not really unconstrained, if $R$ has to be in $SO(3)$; could be made unconstrained via Lie algebra coordinates and applying $\exp$).</p><p>But this is essentially just a different parametrization of the above, and can also be expressed by a cost function.</p><p><img data-proofer-ignore data-src="https://github.com/lacie-life/lacie-life.github.io/blob/main/assets/img/post_assest/BundlerAdjustment.png?raw=true" alt="Fig.2" /></p><h3 id="degenerate-configurations">Degenerate Configurations</h3><p>i.e. ones where the 8-point alg. provides no unique solution: Here all 8 points lie on a so-called “critical” 2D surface. Can be described by quadratic equations =&gt; <em>quadratic surfaces</em>. Mostly pathological, except for one case: all points lie on a 2D plane.</p><p>Non-degenerate configurations are called <em>general configurations</em>.</p><h3 id="four-point-algorithm">Four-Point Algorithm</h3><p>To handle planes, we use this algorithm instead.</p><h5 id="planar-homographies">Planar Homographies</h5><p>For a point $X$ on the plane with normal $N \in \mathbb{S}^2$, we have $N^\top X = d$ (d = distance of plane from the origin). Assume this holds for $X_1$ from frame 1; then for $X_2$ from frame 2: \(X_2 = RX_1 + T = (R + \tfrac{1}{d} T N^\top) X_1 =: H X_1,\) where $H \in \mathbb{R}^3$ is called <em>homography matrix</em>.</p><p>With 2D coords $x_1, x_2$: $\lambda x_2 = H \lambda_1 x_1$ (called planar homography). Multiplying with $\widehat{x_2}$ yields:</p>\[\widehat{x_2} H x_1 = 0 \quad \text{(planar epipolar (or homography) constraint)}\]<p>With $a := x_1 \otimes \widehat{x_2} \in \mathbb{R}^{9 \times 3}$, we get the equation \(a^T H^s = 0\)</p><h5 id="four-point-algorithm-formulation">Four-Point Algorithm Formulation</h5><ol><li>Calculate $\chi = (a^1, \dots, a^n)^\top \in \mathbb{R}^{3n \times 9}$<li>Compute solution $H^s$ for equation $\chi H^s = 0$ via SVD<li>From $H = R + \tfrac{1}{d} T N^\top$, extract the motion parameters (more difficult!)<ol><li>we can obtain: $R$, $N$, and $T/d$. So again: scale ambiguity.</ol></ol><h5 id="relationships-between-h-and-e">Relationships between $H$ and $E$</h5><p>Numerous relations, in particular $E = \hat{T}H$ and $H^\top E + E^\top H = 0$.</p><h3 id="reconstruction-with-uncalibrated-camera">Reconstruction with Uncalibrated Camera</h3><p>We cannot assume in general that we know the calibration matrix $K$, i.e. for internet images!</p><p>Rewrite the epipolar constraint: Replace $x_1$ with $K^{-1} x_1’$, $x_2$ with $K^{-1} x_2’$. Then =&gt; \(x_2'^\top F x_1' = 0, ~~F := (K^{-1})^\top E K^{-1} \quad \text{(Epipolar constraint for uncalibrated cameras)}\)</p><p>$F$ is called the <em>fundamental matrix</em>. It has an SVD $U \text{diag}(\sigma_1, \sigma_2, 0) V^\top$ (in fact, it can be an arbitrary rank 2 matrix; space much larger than $\mathcal{E}$).</p><h5 id="uncalibrated-reconstruction-and-limitations">Uncalibrated Reconstruction and Limitations</h5><p>8-point algorithm can be extended to estimate $F$ instead of $E$. But a given $F$ can not uniquely (even up to scale) be decomposed into $R, T, K$.</p><p>Possible: find <em>projective reconstructions</em>, i.e. geometry reconstructions defined up to projective transformations =&gt; find <em>canonical reconstruction</em> from these. But in reality, one rather calibrates the camera instead.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/skill/'>Skill</a>, <a href='/categories/computer-vision/'>Computer Vision</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/theory/" class="post-tag no-text-decoration" >Theory</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Computer Vision Fundamental - [Part 5] - Life Zero Blog&url=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Computer Vision Fundamental - [Part 5] - Life Zero Blog&u=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Computer Vision Fundamental - [Part 5] - Life Zero Blog&url=https://lacie-life.github.io/posts/Computer-Vision-Fundamental-5/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink('', 'Link copied successfully!')" data-toggle="tooltip" data-placement="top" title="Copy link"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/ZED2-and-ORB-SLAM3/">ZED2 with ORB-SLAM3 (Stereo-IMU mode) step-by-step</a><li><a href="/posts/Callback-Function-in-C++/">Designing Callbacks in C++</a><li><a href="/posts/ML-for-3D-Geometry-4/">ML for 3D Geometry - Part 4</a><li><a href="/posts/ML-for-3D-Geometry-5/">ML for 3D Geometry - Part 5</a><li><a href="/posts/ML-for-3D-Geometry-10/">ML for 3D Geometry - Part 10</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Computer-Vision-Fundamental-1/"><div class="card-body"> <span class="timeago small" >Jun 10, 2022<i class="unloaded">2022-06-10T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 1]</h3><div class="text-muted small"><p> These notes are lecture notes on the Computer Vision II - Multiple View Geometry course held in the summer term 2021 by Prof. Daniel Cremers/Prof. Florian Bernard. Ref Chapter 1 - Mathematical Ba...</p></div></div></a></div><div class="card"> <a href="/posts/Computer-Vision-Fundamental-2/"><div class="card-body"> <span class="timeago small" >Jun 11, 2022<i class="unloaded">2022-06-11T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 2]</h3><div class="text-muted small"><p> Chapter 2 - Representing a Moving Scene Origins of 3D Reconstruction 3D reconstruction is a classical ill-posed problem, as its solutions are not unique (most extreme example: imagine a photograph...</p></div></div></a></div><div class="card"> <a href="/posts/Computer-Vision-Fundamental-3/"><div class="card-body"> <span class="timeago small" >Jun 12, 2022<i class="unloaded">2022-06-12T11:11:11+07:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Computer Vision Fundamental - [Part 3]</h3><div class="text-muted small"><p> Chapter 3 - Perspective Projection Goal of MVG: invert the image formation process. One part of the formation process is the camera motion (last lecture). The second one is the projection of point...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Computer-Vision-Fundamental-4/" class="btn btn-outline-primary" prompt="Older"><p>Computer Vision Fundamental - [Part 4]</p></a> <a href="/posts/Computer-Vision-Fundamental-6/" class="btn btn-outline-primary" prompt="Newer"><p>Computer Vision Fundamental - [Part 6]</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Life Zero</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/tutorial/">tutorial</a> <a class="post-tag" href="/tags/machine-learning/">Machine Learning</a> <a class="post-tag" href="/tags/theory/">Theory</a> <a class="post-tag" href="/tags/writting/">writting</a> <a class="post-tag" href="/tags/project/">Project</a> <a class="post-tag" href="/tags/note/">note</a> <a class="post-tag" href="/tags/collection/">collection</a> <a class="post-tag" href="/tags/cuda/">CUDA</a> <a class="post-tag" href="/tags/opencv/">openCV</a> <a class="post-tag" href="/tags/paper/">paper</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://lacie-life.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
